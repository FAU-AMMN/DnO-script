

<!DOCTYPE html>


<html lang="de" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>1.3. Verfahren der konjugierten Gradienten &#8212; Diskretisierung und numerische Optimierung</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/katex.min.js"></script>
    <script src="../../_static/auto-render.min.js"></script>
    <script src="../../_static/katex_autorenderer.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optimierung/02_NichtlineareOptimierung/03_02_NichtlineareOptimierung';</script>
    <link rel="index" title="Stichwortverzeichnis" href="../../genindex.html" />
    <link rel="search" title="Suche" href="../../search.html" />
    <link rel="next" title="1.4. Wahl der Schrittweite" href="../03_NichtlineareOptimierung2/00_03_NichtlineareOptimierung2.html" />
    <link rel="prev" title="1.2. Abstiegsverfahren" href="02_02_NichtlineareOptimierung.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="de"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../01_Einleitung.html">
  
  
  
  
  
  
    <p class="title logo__title">Diskretisierung und numerische Optimierung</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../01_Einleitung.html">
                    Einleitung
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_02_NichtlineareOptimierung.html">1. Numerische Optimierung</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_02_NichtlineareOptimierung.html">1.1. Mathematische Grundlagen</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_02_NichtlineareOptimierung.html">1.2. Abstiegsverfahren</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.3. Verfahren der konjugierten Gradienten</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_NichtlineareOptimierung2/00_03_NichtlineareOptimierung2.html">1.4. Wahl der Schrittweite</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03_NichtlineareOptimierung2/01_03_NichtlineareOptimierung2.html">1.5. Nicht-differenzierbare Optimierung</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ode/04_Anfangswertprobleme/00_04_Anfangswertprobleme.html">2. Numerische Lösungsverfahren für Anfangswertprobleme</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ode/04_Anfangswertprobleme/01_04_Anfangswertprobleme.html">2.1. Theorie für Anfangswertprobleme gewöhnlicher Differentialgleichungen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ode/04_Anfangswertprobleme/02_04_Anfangswertprobleme.html">2.2. Einschrittverfahren für Anfangswertprobleme</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ode/04_Anfangswertprobleme/03_04_Anfangswertprobleme.html">2.3. Mehrschrittverfahren für Anfangswertprobleme</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ode/04_Anfangswertprobleme/04_04_Anfangswertprobleme.html">2.4. Weiterführende Themen</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ode/05_Randwertprobleme/00_05_Randwertprobleme.html">3. Numerische Lösung von Randwertproblemen</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ode/05_Randwertprobleme/01_05_Randwertprobleme.html">3.1. Existenz und Eindeutigkeit von Lösungen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ode/05_Randwertprobleme/02_05_Randwertprobleme.html">3.2. Differenzenverfahren für Randwertprobleme</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../references.html">4. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/FAU-AMMN/MathPhysicsC" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Quell-Repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Laden Sie diese Seite herunter">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/optimierung/02_NichtlineareOptimierung/03_02_NichtlineareOptimierung.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Quelldatei herunterladen"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="In PDF drucken"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Vollbildmodus"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Suche" aria-label="Suche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Verfahren der konjugierten Gradienten</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Inhalt </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problemstellung">1.3.1. Problemstellung</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1.3.2. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonale-abstiegsrichtungen">1.3.3. Orthogonale Abstiegsrichtungen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#konjugierte-abstiegsrichtungen">1.3.4. Konjugierte Abstiegsrichtungen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#konjugierte-gradienten">1.3.5. Konjugierte Gradienten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verallgemeinerung-fur-nichtlineare-optimierung">1.3.6. Verallgemeinerung für nichtlineare Optimierung</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="verfahren-der-konjugierten-gradienten">
<span id="s-cg-verfahren"></span><h1><span class="section-number">1.3. </span>Verfahren der konjugierten Gradienten<a class="headerlink" href="#verfahren-der-konjugierten-gradienten" title="Permalink to this heading">#</a></h1>
<p>Im Folgenden wollen wir uns mit einem besonders eleganten Verfahren der
Optimierung beschäftigen: dem Verfahren der konjugierten Gradienten.
Ursprünglich wurde das Verfahren von Hestenes und Stiefel in
<span id="id1">[<a class="reference internal" href="../../references.html#id12" title="Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 1952.">HS52</a>]</span> im Jahr <span class="math">\(1952\)</span> vorgeschlagen. Obwohl das
Verfahren im Allgemeinen für die nichtlineare Optimierung eingesetzt
werden kann, wird es insbesondere zur Lösung von großen linearen
Gleichungssystemen <span class="math">\(Ax = b\)</span> mit symmetrischer, dünn besetzter, positiv
definiter Matrix <span class="math">\(A \in \R^{n\times n}\)</span> eingesetzt. Solche
Gleichungssysteme treten zum Beispiel bei der numerischen Modellierung
und Lösung partieller Differentialgleichungen auf. Das Verfahren lässt
sich in diesem Fall besonders anschaulich motivieren und herleiten.
Darum wollen wir uns im Folgenden zunächst auf das Lösen von großen
linearen Gleichungssystemen <span class="math">\(Ax=b\)</span> konzentrieren. Wir folgen bei der
Herleitung des Verfahren der konjugierten Gradienten der didaktisch sehr
gelungenen Arbeit von Jonathan Shewchuk in <span id="id2">[<a class="reference internal" href="../../references.html#id11">She94</a>]</span>. Für
eine ansprechende, interaktive Visualisierung des Verfahren der
konjugierten Gradienten empfehlen wir den Mathematik Blog von Philipp
Wacker <span id="id3">[<a class="reference internal" href="../../references.html#id8" title="Philipp Wacker. Mathematik blog „all about that bayes\grqq . URL: https://de.wikipedia.org/wiki/N-K%C3%B6rper-Problem (visited on 2023-05-03).">Wac</a>]</span>.</p>
<section id="problemstellung">
<span id="ss-cg-problemstellung"></span><h2><span class="section-number">1.3.1. </span>Problemstellung<a class="headerlink" href="#problemstellung" title="Permalink to this heading">#</a></h2>
<p>Sei im Folgenden also <span class="math">\(A \in \mathbb{R}^{n \times n}\)</span> eine sehr große
Matrix und <span class="math">\(b \in \mathbb{R}^n\)</span> ein reeller Vektor. Wir suchen einen
unbekannten Vektor <span class="math">\(x \in \mathbb{R}^n\)</span>, der das lineare
Gleichungssystem</p>
<div class="math" id="equation-eq-lgs">
<span class="eqno">(1.30)<a class="headerlink" href="#equation-eq-lgs" title="Permalink to this equation">#</a></span>\[Ax \ = \ b\]</div>
<p>löst. Wir suchen also nach denjenigen Koeffizienten, mit denen sich der
Vektor <span class="math">\(b\)</span> als Linearkombination aus Spaltenvektoren der Matrix <span class="math">\(A\)</span>
darstellen lässt. Diese Koeffizienten entsprechen den Einträgen des
unbekannten Vektors <span class="math">\(x\)</span>.</p>
<p>Aus der Vorlesung Einführung in die Numerik  in <span id="id4">[<a class="reference internal" href="../../references.html#id3" title="Daniel Tenbrinck and Tim Roith. Vorlesungsskript zur einführung in die numerik (ws 22/23) an der fau erlangen-nürnberg. URL: https://www.math.fau.de/wp-content/uploads/2023/05/tenbrinck_script_numerik.pdf (visited on 2023-05-23).">TR</a>]</span> ist
bekannt, dass es genau dann eine eindeutige Lösung <span class="math">\(x \in \mathbb{R}^n\)</span>
für die Gleichung <a class="reference internal" href="#equation-eq-lgs">(1.30)</a> gibt, falls die Determinante
<span class="math">\(\operatorname{det}(A) \neq 0\)</span> ist. Eine hinreichende Bedingung für die
Eindeutigkeit des Lösungsvektors <span class="math">\(x \in \mathbb{R}^n\)</span> ist es also zu
fordern, dass die Matrix <span class="math">\(A\)</span> symmetrisch und positiv definit ist.</p>
<p>Wir gehen aus diesem Grund im Folgenden immer davon aus, dass
<span class="math">\(A \in \mathbb{R}^{n\times n}\)</span> eine <em>symmetrische</em> und <em>positiv
definite</em> Matrix ist. In diesem Fall ist das Bestimmen einer Lösung von
<a class="reference internal" href="#equation-eq-lgs">(1.30)</a> ein gut-gestelltes Problem und die Lösung lässt sich direkt
angeben als:</p>
<div class="math">
\[x \ = \ A^{-1}b.\]</div>
<p>Wie wir jedoch ebenfalls aus <span id="id5">[<a class="reference internal" href="../../references.html#id3" title="Daniel Tenbrinck and Tim Roith. Vorlesungsskript zur einführung in die numerik (ws 22/23) an der fau erlangen-nürnberg. URL: https://www.math.fau.de/wp-content/uploads/2023/05/tenbrinck_script_numerik.pdf (visited on 2023-05-23).">TR</a>]</span> wissen ist die Inversion
einer Matrix <span class="math">\(A \in \mathbb{R}^{n\times n}\)</span> numerisch sehr aufwänding
und selbst unter Ausnutzung der Symmetrie lässt sich höchstens ein
Verfahren mit Rechenaufwand <span class="math">\(\mathcal{O}(\frac{1}{6}n^3)\)</span> angeben.
Sollte die Dimension des Problems jedoch sehr groß sein (d.h. wir nehmen
<span class="math">\(n &gt;&gt; 1\)</span> an), so ist eine direkte Lösung von <a class="reference internal" href="#equation-eq-lgs">(1.30)</a> mittels
Inversion nicht durchführbar. Glücklicherweise liefert uns das Verfahren
der konjugierten Gradienten (neben anderen iterativen
Lösungsalgorithmen) eine Möglichkeit das lineare Gleichungssystem
numerisch zu lösen. Man beachte, dass wir explizit darauf verzichten zu
fordern, dass die Matrix <span class="math">\(A\)</span> <em>dünnbesetzt</em> ist. In diesem Fall könnten
wir nämlich ebenfalls die numerischen Iterationsverfahren aus
<span id="id6">[<a class="reference internal" href="../../references.html#id3" title="Daniel Tenbrinck and Tim Roith. Vorlesungsskript zur einführung in die numerik (ws 22/23) an der fau erlangen-nürnberg. URL: https://www.math.fau.de/wp-content/uploads/2023/05/tenbrinck_script_numerik.pdf (visited on 2023-05-23).">TR</a>]</span> anwenden.</p>
<p>Wir betrachten zunächst das folgende konvexe, quadratische
Optimierungsproblem der Form</p>
<div class="math" id="equation-eq-quadratic-problem">
<span class="eqno">(1.31)<a class="headerlink" href="#equation-eq-quadratic-problem" title="Permalink to this equation">#</a></span>\[\min_{x\in \mathbb{R}^n} \, \frac{1}{2} \langle x, Ax \rangle - \langle b, x \rangle + c,\]</div>
<p>wobei <span class="math">\(A\)</span> und <span class="math">\(b\)</span> wie im Fall des linearen Gleichungssystems in
<a class="reference internal" href="#equation-eq-lgs">(1.30)</a> gewählt sind und <span class="math">\(c \in \mathbb{R}\)</span> eine beliebige, reelle
Konstante ist. Der folgende Satz liefert uns eine hilfreiche Aussage zur
Lösung des ursprünglichen Problems.</p>
<div class="proof theorem admonition" id="thm:LGS_equivalent">
<p class="admonition-title"><span class="caption-number">Theorem 1.6 </span> (Äquivalenzaussage für lineares Gleichungssystem)</p>
<section class="theorem-content" id="proof-content">
<p>Das konvexe, quadratische Minimierungsproblem in
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> ist äquivalent zum ursprünglichen linearen
Gleichungssystem in <a class="reference internal" href="#equation-eq-lgs">(1.30)</a>, d.h., jede Lösung von
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> ist schon Lösung von <a class="reference internal" href="#equation-eq-lgs">(1.30)</a> und anders
herum.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Für die erste Richtung des Beweises nehmen wir an, dass
<span class="math">\(x^* \in \mathbb{R}^n\)</span> eine Lösung des linearen Gleichungssystems
<span class="math">\(Ax = b\)</span> sei. Wir betrachten die hinreichenden Optimalitätsbedingungen
zweiter Ordnung aus <a class="reference internal" href="01_02_NichtlineareOptimierung.html#thm:minimum_hinreichend">Theorem 1.3</a> für das
Minimierungsproblem <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a>. Hierzu suchen wir
zunächst die stationären Punkte der Funktion
<span class="math">\(F \colon \mathbb{R}^n \rightarrow \mathbb{R}\)</span> mit</p>
<div class="math">
\[F(x) \ \coloneqq \ \frac{1}{2} \langle x, Ax \rangle - \langle b, x \rangle + c.\]</div>
<p>Der Gradient von <span class="math">\(F\)</span> lässt sich wegen der Symmetrie von <span class="math">\(A\)</span> bestimmen
als</p>
<div class="math">
\[\nabla F(x) \ = \ \frac{1}{2}(A + A^T)x - b \ = \ Ax - b \ \overset{!}{=} \ 0.\]</div>
<p>Alle stationären Punkte <span class="math">\(x \in \mathbb{R}^n\)</span> von <span class="math">\(F\)</span> mit
<span class="math">\(\nabla F(x) = 0\)</span> sind also gerade die Lösungen des linearen
Gleichungssystems <span class="math">\(Ax = b\)</span>. Damit ist <span class="math">\(x^*\)</span> nach Vorraussetzung also
einziger stationärer Punkt von <span class="math">\(F\)</span>. Um zu zeigen, dass
<span class="math">\(x^* \in \mathbb{R}^n\)</span> auch schon ein lokales Minimum von <span class="math">\(F\)</span> ist müssen
wir noch die Hessematrix von <span class="math">\(F\)</span> betrachten, welche gegeben ist durch:</p>
<div class="math">
\[\nabla^2 F(x) \ = \ A.\]</div>
<p>Da <span class="math">\(A\)</span> nach Vorraussetzung positiv definit ist, ist auch die Hessematrix
<span class="math">\(\nabla^2 F(x) = A\)</span> positiv definit und somit sind die hinreichenden
Kriterien für das Vorliegen eines lokalen Minimums von <span class="math">\(F\)</span> im Punkt
<span class="math">\(x^* \in \mathbb{R}^n\)</span> erfüllt.</p>
<p>Für die Rückrichtung des Beweises nehmen wir, dass
<span class="math">\(x^* \in \mathbb{R}^n\)</span> ein lokales Minimum der Zielfunktion <span class="math">\(F\)</span> ist.
Damit folgt direkt, dass <span class="math">\(x^*\)</span> ein stationärer Punkt von <span class="math">\(F\)</span> ist und
somit muss gelten:</p>
<div class="math">
\[\nabla F(x^*) \ = \ A x^* - b \ = \ 0.\]</div>
<p>Das bedeutet aber schon, dass <span class="math">\(x^* \in \mathbb{R}^n\)</span> Lösung des linearen
Gleichungssystems <span class="math">\(Ax = b\)</span> ist. ◻</p>
</div>
<p>Der <a class="reference internal" href="#thm:LGS_equivalent">Theorem 1.6</a> erlaubt es uns also ein quadratisches
Optimierungsproblem der Form <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> numerisch zu
lösen anstatt einen unbekannten Lösungsvektor für ursprüngliche lineare
Gleichungssystem <a class="reference internal" href="#equation-eq-lgs">(1.30)</a> zu finden.</p>
<p>Wir interessieren uns nun also für ein iteratives Verfahren, welches
eine Folge von Punkten <span class="math">\(x_0,x_1,\ldots \in \mathbb{R}^n\)</span> konstruiert,
die gegen ein Minimum von <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> und somit gegen die
eindeutige Lösung des linearen Gleichungssystems <a class="reference internal" href="#equation-eq-lgs">(1.30)</a>
konvergiert. Hierfür benötigen wir noch zusätzliche Notation, um das
angestrebte Verfahren vernünftig zu beschreiben.</p>
<div class="proof definition admonition" id="def:residuum">
<p class="admonition-title"><span class="caption-number">Definition 1.7 </span> (Fehler und Residuum)</p>
<section class="definition-content" id="proof-content">
<p>Sei <span class="math">\(x_{k+1} = G(x_k)\)</span> ein Iterationsverfahren, dass gegen ein lokales
Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> der quadratischen Funktion <span class="math">\(F\)</span> in
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> konvergiert, d.h., <span class="math">\(x_k \rightarrow x^*\)</span> für
<span class="math">\(k \rightarrow \infty\)</span>. Dann können wir die beiden folgenden Begriffe
definieren:</p>
<ol class="arabic">
<li><p>Wir bezeichnen den Vektor <span class="math">\(e_k \in \mathbb{R}^n\)</span> mit</p>
<div class="math">
\[e_k \ \coloneqq \ x_k - x^*\]</div>
<p>als den aktuellen <strong>Fehler</strong>, den man durch den aktuellen Punkt
<span class="math">\(x_k \in \mathbb{R}^n\)</span> macht.</p>
</li>
<li><p>Wir bezeichnen den Vektor <span class="math">\(r_k \in \mathbb{R}^n\)</span> mit</p>
<div class="math">
\[r_k \ \coloneqq \ b - Ax_k\]</div>
<p>als das aktuelle <strong>Residuum</strong>, das man durch den aktuellen Punkt
<span class="math">\(x_k \in \mathbb{R}^n\)</span> erhält.</p>
</li>
</ol>
</section>
</div><div class="proof remark admonition" id="rem:fehler_residuum">
<p class="admonition-title"><span class="caption-number">Remark 1.12 </span> (Fehler und Residuum)</p>
<section class="remark-content" id="proof-content">
<p>In Bezug auf <a class="reference internal" href="#def:residuum">Definition 1.7</a> lassen sich folgende Aussagen
festhalten:</p>
<ol class="arabic">
<li><p>Der Fehler <span class="math">\(e_k \in \mathbb{R}^n\)</span> ist eher abstrakter Natur und
dient zur besseren Analyse des Verfahrens der konjugierten
Gradienten. Explizit werden wir diesen Vektor jedoch nie bestimmen
können innerhalb des Iterationsverfahrens, da wir dann schon fertig
wären mit einem einfachen Update der Form <span class="math">\(x^* = x_k - e_k\)</span>.</p></li>
<li><p>Wie wir bereits im Beweis von <a class="reference internal" href="#thm:LGS_equivalent">Theorem 1.6</a> gesehen
haben, lässt sich das Residuum <span class="math">\(r_k \in \mathbb{R}^n\)</span> außerdem wie
folgt umschreiben:</p>
<div class="math">
\[r_k \ = \ \underbrace{b - Ax_k}_{= -\nabla F(x_k)} \ = \ Ax^* - Ax_k \ = \ A (x^* - x_k) \ = \ -Ae_k.\]</div>
<p>Daher lässt sich das Residuum <span class="math">\(r_k\)</span> auch als die Richtung des
stärksten Abstiegs interpretieren und es ist klar, dass <span class="math">\(r_k\)</span> immer
orthogonal zu den Niveaulinien der Funktion <span class="math">\(F\)</span> steht.</p>
</li>
</ol>
</section>
</div><p><a class="reference internal" href="#fig-residuum"><span class="std std-numref">Abb. 1.2</span></a> illustriert anschaulich die geometrische
Bedeutung der beiden in <a class="reference internal" href="#def:residuum">Definition 1.7</a> eingeführten Vektoren.
Wie man unschwer erkennt zeigen Fehler und Residuum im Allgemeinen nicht
in die selbe Richtung. Das erklärt auch warum das
Gradientenabstiegsverfahren in <a class="reference internal" href="02_02_NichtlineareOptimierung.html#ss-gradient-descent"><span class="std std-ref">Gradientenabstiegsverfahren</span></a> selbst bei
optimaler Schrittweite <span class="math">\(\alpha_k &gt; 0\)</span> nicht in einem Schritt die
gesuchte Lösung <span class="math">\(x^* \in \mathbb{R}^n\)</span> erreicht.</p>
<figure class="align-default" id="fig-residuum">
<img alt="../../_images/residuum.png" src="../../_images/residuum.png" />
<figcaption>
<p><span class="caption-number">Abb. 1.2 </span><span class="caption-text">Visualisierung des Fehlers <span class="math">\(e_0 \in \mathbb{R}^n\)</span> und des Residuums
<span class="math">\(r_0 \in \mathbb{R}^n\)</span> für einen Startpunkt <span class="math">\(x_0 \in \mathbb{R}^n\)</span>.</span><a class="headerlink" href="#fig-residuum" title="Link zu diesem Bild">#</a></p>
</figcaption>
</figure>
</section>
<section id="motivation">
<span id="ss-motivation"></span><h2><span class="section-number">1.3.2. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">#</a></h2>
<p>Um das Vorgehen beim Verfahren der konjugierten Gradienten zu motivieren
rufen wir uns noch einmal das Gradientenabstiegsverfahren aus
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#ss-gradient-descent"><span class="std std-ref">Gradientenabstiegsverfahren</span></a> in Erinnerung. Nehmen wir an wir befinden uns
im <span class="math">\(k\)</span>-Schritt des Gradientenabstiegsverfahrens in Algorithmus
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#alg:gradient_descent_adaptive">Algorithm 1.2</a> in einem Punkt
<span class="math">\(x_k \in \mathbb{R}^n\)</span> und es sei eine Schrittweite <span class="math">\(\alpha_k &gt; 0\)</span>
gegeben. Dann erhalten wir den nächsten Punkt <span class="math">\(x_{k+1} \in \mathbb{R}^n\)</span>
der Iterationsfolge durch folgendes Update:</p>
<div class="math">
\[x_{k+1} \ = \ x_k - \alpha_k \nabla F(x_k) \ = \ x_k + \alpha_k r_k,\]</div>
<p>wobei <span class="math">\(r_k \in \mathbb{R}^n\)</span> das aktuelle Residuum des Punktes <span class="math">\(x_k\)</span>
bezeichnet. Wir machen also in Richtung des steilsten Gradientenabstiegs
einen Schritt der Länge <span class="math">\(\alpha_k &gt; 0\)</span>. Da die Abstiegsrichtung in jedem
Schritt <span class="math">\(x_k \rightarrow x_{k+1}\)</span> orthogonal zu den Niveaulinien von <span class="math">\(F\)</span>
steht, erhält man typischerweise einen Zickzack-Pfad durch das
Gradientenabstiegsverfahren (vgl.
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#fig-gradient-descent-adaptive"><span class="std std-numref">Abb. 1.1</span></a>).</p>
<p>Um dieses typische Verhalten besser zu verstehen können wir eine
Vorüberlegung zur Schrittweitenwahl für das quadratische
Optimierungsproblem in <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> machen. Hierzu gehen
wir analog zur Bestimmung der optimalen Schrittrichtung in
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#ss-gradient-descent"><span class="std std-ref">Gradientenabstiegsverfahren</span></a> vor, nur dass wir diesmal die Schrittrichtung
<span class="math">\(p_k \coloneqq -\nabla F(x_k)\)</span> festhalten und bezüglich der unbekannten
Schrittweite optimieren.</p>
<p>Wir gehen davon aus, dass wir das lokale Minimum von <span class="math">\(F\)</span> noch nicht
erreicht haben, denn dann wäre <span class="math">\(\alpha_k = 0\)</span>. Wir suchen also eine
Schrittweite <span class="math">\(\alpha &gt; 0\)</span>, so dass der Funktionswert <span class="math">\(F(x_{k+1})\)</span>
entlang der Linie <span class="math">\(x_k - \alpha \nabla F(x_k)\)</span> minimal wird. Da <span class="math">\(F\)</span> eine
quadratische Funktion ist, wissen wir, dass ein eindeutiges Minimum
<span class="math">\(\alpha_k\)</span> entlang dieser Linie existieren muss. Wir nutzen also die
notwendigen Optimalitätsbedingungen aus <a class="reference internal" href="01_02_NichtlineareOptimierung.html#thm:minimum_notwendig">Theorem 1.1</a>
für das totale Differential, um folgenden Zusammenhang herzustellen:</p>
<div class="math">
\[\begin{split}
\frac{\mathrm{d}}{\mathrm{d}\alpha} F(x_{k+1}) \ &= \ \Bigl\langle \nabla F(x_{k+1}), \frac{\mathrm{d}x_{k+1}}{\mathrm{d}\alpha} \Bigr\rangle \ = \ \Bigl\langle \nabla F(x_{k+1}), \frac{\mathrm{d} (x_k - \alpha \nabla F(x_k))}{\mathrm{d}\alpha} \Bigr\rangle \\ 
\ &= \ \langle \nabla F(x_{k+1}), -\nabla F(x_k) \rangle \ = \  \langle \nabla F(x_{k+1}), p_k \rangle \ \overset{!}{=} \ 0.
\end{split}\]</div>
<p>Das Ergebnis ist durchaus interessant. Die optimale Schrittweite
<span class="math">\(\alpha &gt; 0\)</span> muss so gewählt werden, dass der nächste Punkt
<span class="math">\(x_{k+1} \in \mathbb{R}^n\)</span> der Iterationsfolge an der Stelle liegt an
der unsere Abstiegsrichtung orthogonal auf den Gradienten der Funktion
<span class="math">\(\nabla F(x_{k+1})\)</span> trifft. Das bedeutet, dass die optimale Abfolge der
Abstiegsrichtungen im quadratischen Fall eine Menge von <span class="math">\(90\)</span>-Grad
Zickzack-Linien ergibt, was zu unseren Beobachtungen in
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#fig-gradient-descent-adaptive"><span class="std std-numref">Abb. 1.1</span></a> passt. Da jedoch der Punkt
<span class="math">\(x_{k+1} \in \mathbb{R}^n\)</span> bislang noch unbekannt ist, können wir das
optimale <span class="math">\(\alpha_k\)</span> nicht in dieser Form angeben. Das folgende Lemma
bestimmt die optimale Schrittweite im Fall der quadratischen Optimierung
in <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a>.</p>
<div class="proof lemma admonition" id="lem:optimale_schrittweite">
<p class="admonition-title"><span class="caption-number">Lemma 1.1 </span> (Optimale Schrittweite)</p>
<section class="lemma-content" id="proof-content">
<p>Sei <span class="math">\(F \colon \mathbb{R}^n \rightarrow \mathbb{R}\)</span> die quadratische
Funktion aus <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a>. Wir betrachten das
Gradientenabstiegsverfahren im <span class="math">\(k\)</span>-ten Iterationsschritt mit einer
unbekannten Schrittweite <span class="math">\(\alpha_k &gt; 0\)</span>, die jedoch so gewählt werden
muss, dass</p>
<div class="math">
\[\langle \nabla F(x_{k+1}), \nabla F(x_k) \rangle \ \overset{!}{=} \ 0.\]</div>
<p>Sei außerdem <span class="math">\(r_k = b - Ax_k\)</span> das Residuum im aktuellen
Iterationsschritt. Dann lässt sich die optimale Schrittweite <span class="math">\(\alpha_k\)</span>
berechnen als:</p>
<div class="math" id="equation-eq-optimal-step-size">
<span class="eqno">(1.32)<a class="headerlink" href="#equation-eq-optimal-step-size" title="Permalink to this equation">#</a></span>\[\alpha_k \ = \ \frac{\langle r_k, r_k \rangle}{\langle r_k, Ar_k \rangle}.\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Wir erinnern uns daran, dass <span class="math">\(r_k = -\nabla F(x_k) = b - Ax_{k}\)</span> ist und
somit können wir folgern:</p>
<div class="math">
\[\begin{split}
0 \ &\overset{!}{=} \ \langle \nabla F(x_{k+1}), \nabla F(x_k) \rangle \ = \ \langle r_{k+1}, r_k \rangle \ = \ \langle b - Ax_{k+1}, r_k \rangle \\
 \ &= \ \langle b - A(x_k + \alpha_kr_k), r_k \rangle \ = \  \langle b - Ax_k, r_k \rangle - \alpha_k \langle Ar_k, r_k \rangle \\
 \ &= \langle r_k, r_k \rangle - \alpha_k \langle r_k, Ar_k \rangle
\end{split}\]</div>
<p>Da wir <span class="math">\(A\)</span> als positiv definit angenommen haben, können wir die
Gleichung umstellen und erhalten so die behauptete Berechnungsformel für
<span class="math">\(\alpha_k\)</span> in <a class="reference internal" href="#equation-eq-optimal-step-size">(1.32)</a>. ◻</p>
</div>
<p>Obwohl wir die optimale Schrittweite <span class="math">\(\alpha_k\)</span> in
<a class="reference internal" href="02_02_NichtlineareOptimierung.html#equation-eq-optimal-direction">(1.6)</a> für das quadratische Optimierungsproblem
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> bestimmen konnten ist das
Gradientenabstiegsverfahren weit davon entfernt optimal zu sein. Trotz
optimaler Schrittweiten und optimaler Abstiegsrichtungen erhalten wir
eine Folge von Richtungsvektoren, die immer wieder in die gleiche
Richtung zeigen (siehe <a class="reference internal" href="#fig-zig-zag"><span class="std std-numref">Abb. 1.3</span></a>). Das ist numerisch gesehen
äußerst ineffizient. Man könnte sich also fragen, warum man nicht
einfach nur zwei orthogonale Schritte macht und die Schrittweiten als
Summe der optimalen Schrittweiten der geraden bzw. ungeraden
Iterationsschritte <span class="math">\(k \in \mathbb{N}\)</span> wählt. In der Tat würde man für
<span class="math">\(N \in \mathbb{N}\)</span> Schritte des Gradientenabstiegsverfahren im selben
Punkt <span class="math">\(x_N \in \mathbb{R}^n\)</span> mit nur zwei Iterationen landen, wie in
<a class="reference internal" href="#fig-zig-zag"><span class="std std-numref">Abb. 1.3</span></a> illustriert ist.</p>
<figure class="align-default" id="fig-zig-zag">
<img alt="../../_images/residuum.png" src="../../_images/residuum.png" />
<figcaption>
<p><span class="caption-number">Abb. 1.3 </span><span class="caption-text">Vergleich des Gradientenabstiegsverfahrens mit optimaler Schrittweite
<span class="math">\(\alpha_k &gt; 0\)</span> aus <a class="reference internal" href="#lem:optimale_schrittweite">Lemma 1.1</a> (links) mit
einem idealen Abstiegsverfahren (rechts), bei dem alle orthogonalen
Teilschritte zusammengefasst sind.</span><a class="headerlink" href="#fig-zig-zag" title="Link zu diesem Bild">#</a></p>
</figcaption>
</figure>
<p>Leider können wir nicht alle Schrittweiten aufaddieren, da wir zur
Berechnung der optimalen Schrittlänge <span class="math">\(\alpha_k &gt; 0\)</span> bereits alle
vorangegangen Punkte <span class="math">\(x_k k=0,\ldots,k-1\)</span> kennen müssten. Außerdem würde
ein großer, zusammengefasster Schritt in die erste der Richtungen
eventuell dazu führen, dass man keinen Abstieg der Funktionswerte von
<span class="math">\(F\)</span> entlang der Abstiegsrichtung bis zum Minimum mehr realisiert,
sondern im Allgemeinen dieses Minimum überschreitet. Diese Beobachtung
ist in <a class="reference internal" href="#fig-two-step"><span class="std std-numref">Abb. 1.4</span></a> illustriert.</p>
<figure class="align-default" id="fig-two-step">
<img alt="../../_images/two-step.png" src="../../_images/two-step.png" />
<figcaption>
<p><span class="caption-number">Abb. 1.4 </span><span class="caption-text">Illustration eines idealen Abstiegsverfahrens mit zwei orthogonalen
Richtungen. Man beachte, dass die Schrittweite <span class="math">\(\alpha_0 &gt; 0\)</span> so gewählt
werden muss, dass man im ersten Schritt nicht in einem Punkt
<span class="math">\(x_1 \in \mathbb{R}^2\)</span> mit minimalen Funktionswert <span class="math">\(F(x_1)\)</span> entlang der
Richtung <span class="math">\(x_0 - \alpha_0 \nabla F(x_0)\)</span> endet.</span><a class="headerlink" href="#fig-two-step" title="Link zu diesem Bild">#</a></p>
</figcaption>
</figure>
<p>Die Ideallösung wäre natürlich von einem Startpunkt
<span class="math">\(x_0 \in \mathbb{R}^n\)</span> in nur einem Schritt zum lokalen Optimum
<span class="math">\(x^* \in \mathbb{R}^n\)</span> zu gelangen. Da wir aber den Punkt <span class="math">\(x^*\)</span> a-priori
nicht kennen ist das eine unrealistische Forderung. Dennoch lässt sich
zeigen, dass das Gradientenabstiegsverfahren mit der optimalen
Schrittweite <span class="math">\(\alpha_k\)</span> in <a class="reference internal" href="#equation-eq-optimal-step-size">(1.32)</a> im Fall des
quadratischen Optimierungsproblems <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> in genau
einem Schritt zum lokalen Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> führt, wenn der
Fehler <span class="math">\(e_0 = x_0 - x^*\)</span> ein Eigenvektor von <span class="math">\(A\)</span> ist. Wir wissen nämlich
aus <a class="reference internal" href="#rem:fehler_residuum">Remark 1.12</a>, dass <span class="math">\(r_k = -Ae_k\)</span> gilt und somit
erhalten wir für das Gradientenabstiegsverfahren im ersten Schritt:</p>
<div class="math">
\[x_1 \ = \ x_0 - \alpha_0 \nabla F(x_0) \ = \ x_0 + \alpha_0 r_0 \ = \ x_0 - \alpha_0 A e_0 \ = \ x_0 - \alpha_0 \lambda e_0.\]</div>
<p>Man müsste bei der Wahl des Startpunktes <span class="math">\(x_0\in \R^n\)</span> jedoch viel Glück
haben, um diese Forderung zu erfüllen. Darum wollen wir uns mit
alternativen Ideen beschäftigen.</p>
</section>
<section id="orthogonale-abstiegsrichtungen">
<span id="ss-orthogonal-descent"></span><h2><span class="section-number">1.3.3. </span>Orthogonale Abstiegsrichtungen<a class="headerlink" href="#orthogonale-abstiegsrichtungen" title="Permalink to this heading">#</a></h2>
<p>Wir wünschen uns einen Algorithmus, der ähnlich dem
Gradientenabstiegsverfahren nur orthogonale Richtungen
<span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> mit <span class="math">\(d_i \in \mathbb{R}^n\)</span> für
<span class="math">\(0 \leq i \leq n-1\)</span> verwendet, jedoch mit der Einschränkung, dass diese
nur ein einziges Mal genutzt werden können. Ziel dieses Verfahrens soll
es außerdem sein durch <span class="math">\(n\)</span> Schritte in die jeweils <span class="math">\(n\)</span> orthogonalen
Richtungen <span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> das lokale Minimum der
Funktion zu erreichen. Damit hätten wir ein Itrerationsverfahren der
Form</p>
<div class="math" id="equation-eq-orthogonal-descent">
<span class="eqno">(1.33)<a class="headerlink" href="#equation-eq-orthogonal-descent" title="Permalink to this equation">#</a></span>\[x_{k+1} \ = \ x_k + \alpha_k d_k, \quad \alpha_k > 0, \ k = 0,\ldots,n-1\]</div>
<p>gewonnen. Wir könnten dies erzwingen indem wir im <span class="math">\(k\)</span>-ten Schritt des
Iterationsverfahrens fordern, dass ein Schritt in Richtung
<span class="math">\(d_k \in \mathbb{R}^n\)</span> dazu führt, dass der Fehler
<span class="math">\(e_{k+1} \in \mathbb{R}^n\)</span> keinerlei Komponenten dieser Richtung mehr
enthält, d.h. wir fordern</p>
<div class="math" id="equation-eq-req-error">
<span class="eqno">(1.34)<a class="headerlink" href="#equation-eq-req-error" title="Permalink to this equation">#</a></span>\[\langle e_{k+1}, d_k \rangle \ \overset{!}{=} \ 0.\]</div>
<p>Da wir den zu erwartenden Fehler <span class="math">\(e_{k+1}\)</span> in Bezug auf den aktuellen
Punkt <span class="math">\(x_k \in \mathbb{R}^n\)</span> folgendermaßen umschreiben können:</p>
<div class="math">
\[e_{k+1} \ = \ x_{k+1} - x^* \ = \ x_k + \alpha_k d_k - x^* \ = \ e_k + \alpha_k d_k,\]</div>
<p>können wir die Forderung <a class="reference internal" href="#equation-eq-req-error">(1.34)</a> umformulieren zu:</p>
<div class="math">
\[\langle e_k + \alpha_k d_k, d_k \rangle \ \overset{!}{=} \ 0.\]</div>
<p>Hieraus können wir die optimale Schrittweite <span class="math">\(\alpha_k &gt; 0\)</span> in Richtung
<span class="math">\(d_k \in \mathbb{R}^n\)</span> ableiten als</p>
<div class="math" id="equation-eq-optimal-step-size-orthogonal">
<span class="eqno">(1.35)<a class="headerlink" href="#equation-eq-optimal-step-size-orthogonal" title="Permalink to this equation">#</a></span>\[\alpha_k \ = \ - \frac{\langle e_k, d_k \rangle}{\langle d_k, d_k \rangle}.\]</div>
<p>Obwohl wir in <a class="reference internal" href="#equation-eq-optimal-step-size-orthogonal">(1.35)</a> eine optimale
Schrittweite <span class="math">\(\alpha_k\)</span> für das Verfahren mit orthogonalen
Abstiegsrichtungen in <a class="reference internal" href="#equation-eq-orthogonal-descent">(1.33)</a> bestimmen konnten,
hilft und diese nicht in der praktischen Anwendung des Verfahrens, da
sie von dem unbekannten Fehlervektor <span class="math">\(e_k \in \mathbb{R}^n\)</span>. Dieser
hängt natürlich von der unbekannten Lösung <span class="math">\(x^* \in \mathbb{R}^n\)</span> ab und
wenn wir diese kennen würden, so müssten wir kein iteratives Verfahren
konstruieren. Selbst wenn man den Fehler <span class="math">\(e_k\)</span> weiter rekursiv
umschreibst, so würde man schlussendlich doch bei einer Abhängigkeit des
initialen Fehlers <span class="math">\(e_0\)</span> landen. Wir müssen uns also vorerst von dieser
Idee verabschieden und nach einer alternativen Möglichkeit suchen.</p>
</section>
<section id="konjugierte-abstiegsrichtungen">
<span id="ss-conjugated-descent"></span><h2><span class="section-number">1.3.4. </span>Konjugierte Abstiegsrichtungen<a class="headerlink" href="#konjugierte-abstiegsrichtungen" title="Permalink to this heading">#</a></h2>
<p>Obwohl unsere Idee von orthogonalen Abstiegsrichtungen in
<a class="reference internal" href="#ss-orthogonal-descent"><span class="std std-ref">Orthogonale Abstiegsrichtungen</span></a> nicht zum Ziel geführt hat, so war die Idee
gar nicht schlecht. Das Hauptproblem an der ursprünglichen Idee liegt in
der Forderung <a class="reference internal" href="#equation-eq-req-error">(1.34)</a>, nämlich dass der Fehlervektor
<span class="math">\(e_{k+1}\)</span> orthogonal zur aktuellen Richtung <span class="math">\(d_k\)</span> stehen soll. Diese
Forderung führt nämlich dazu, dass man orthogonale Vektoren erhält, die
nicht an die Geometrie des quadratischen Minimierungsproblems
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> angepasst sind.</p>
<p>Wenn man sich die Niveaulinien der Funktion <span class="math">\(F\)</span> genauer anschaut (siehe
zum Beispiel Abbildung <a class="reference internal" href="#fig-two-step"><span class="std std-numref">Abb. 1.4</span></a>), so erkennt man, dass es
Richtungen gibt entlang derer die Abstiegsrichtung zum lokalen Minimum
<span class="math">\(x^* \in \mathbb{R}^n\)</span> steiler verläuft als entlang der anderen
Richtungen. Die geometrischen Eigenschaften des Graphen von <span class="math">\(F\)</span> sind
maßgeblich durch die Gestalt der Matrix <span class="math">\(A\)</span>, genauer gesagt durch deren
Eigenvektoren bestimmt. Daher wollen wir diese Eigenschaften bei der
Konstruktion eines iterativen Abstiegsverfahren berücksichtigen. Hierzu
führen wir folgendes hilfreiche Konzept ein.</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 1.8 </span> (Konjugierte Vektoren)</p>
<section class="definition-content" id="proof-content">
<p>Sei <span class="math">\(A \in \mathbb{R}^{n\times n}\)</span> eine symmetrische, positiv definite
Matrix und <span class="math">\(u,v \in \mathbb{R}^n / \lbrace 0\rbrace\)</span> zwei Vektoren. Wir
nennen <span class="math">\(v\)</span> und <span class="math">\(w\)</span> <strong>konjugiert bezüglich <span class="math">\(\mathbf{A}\)</span></strong> oder auch
<strong><span class="math">\(\mathbf{A}\)</span>-orthogonal</strong> falls gilt</p>
<div class="math">
\[\langle v, Aw \rangle \ = \ \langle w, Av \rangle \ = \ 0.\]</div>
</section>
</div><p>Anstatt nun also die Orthogonalität unserer Richtungsvektoren
<span class="math">\(\lbrace d_0, \ldots, d_{n-1}\rbrace\)</span> zu erzwingen wie in
<a class="reference internal" href="#ss-orthogonal-descent"><span class="std std-ref">Orthogonale Abstiegsrichtungen</span></a>, fordern wir nun, dass diese Vektoren
konjugiert bezüglich der Matrix <span class="math">\(A\)</span> und damit besser an das Problem
angepasst sind.</p>
<div class="proof remark admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 1.13 </span></p>
<section class="remark-content" id="proof-content">
<p>Es ist leicht einzusehen, dass <span class="math">\(A\)</span>-orthogonal und orthogonal die selbe
Eigenschaft beschreiben, falls die Matrix <span class="math">\(A\)</span> ein Vielfaches der
Einheitsmatrix <span class="math">\(I_n \in \mathbb{R}^{n \times n}\)</span> ist. In diesem Fall ist
das quadratische Optimierungsproblem <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a>
symmetrisch in alle Richtungen.</p>
</section>
</div><p>Anschaulich lässt sich die Forderung nach <span class="math">\(A\)</span>-Orthogonalität auch so
deuten, dass wir ein Paar von Vektoren
<span class="math">\(v,w \in \mathbb{R}^n / \lbrace 0\rbrace\)</span> suchen, welche in einem Winkel
so zueinander stehen, dass wenn man die Niveaulinien der Funktion <span class="math">\(F\)</span>
symmetrisch reskaliert, diese Vektoren anschließend orthogonal
zueinander stehen. Diese Idee ist in Abbildung <a class="reference internal" href="#fig-conjugacy"><span class="std std-numref">Abb. 1.5</span></a>
dargestellt.</p>
<figure class="align-default" id="fig-conjugacy">
<img alt="../../_images/conjugacy.png" src="../../_images/conjugacy.png" />
<figcaption>
<p><span class="caption-number">Abb. 1.5 </span><span class="caption-text">Illustration der Geometrie von konjugierten Vektoren im Referenzsystem
<span class="math">\(\mathbb{R}^2\)</span> (links) und der selben Vektoren in einem symmetrisierten
System bezüglich der Matrix <span class="math">\(A\)</span> (rechts).</span><a class="headerlink" href="#fig-conjugacy" title="Link zu diesem Bild">#</a></p>
</figcaption>
</figure>
<p>Anstatt also ein Abstiegsverfahren der Form <a class="reference internal" href="#equation-eq-orthogonal-descent">(1.33)</a>
mit orthogonalen Vektoren zu verwenden, wollen wir ein Abstiegsverfahren
mit <span class="math">\(A\)</span>-orthogonalen Vektoren <span class="math">\(\lbrace d_0, \ldots, d_{n-1}\rbrace\)</span>
konstruieren, d.h., wir verwenden das Iterationsschema</p>
<div class="math" id="equation-eq-conjugate-descent">
<span class="eqno">(1.36)<a class="headerlink" href="#equation-eq-conjugate-descent" title="Permalink to this equation">#</a></span>\[x_{k+1} \ = \ x_k + \alpha_k d_k, \quad \alpha_k > 0, \ k=0,\ldots,n-1\]</div>
<p>wobei für die Abstiegsrichtungen <span class="math">\(d_k \in \mathbb{R}^n\)</span> gelten soll:</p>
<div class="math">
\[\langle d_i, Ad_j \rangle = 0 \qquad \text{ für } \ 0 \leq i,j \leq n-1 \ \text{ mit } i\neq j.\]</div>
<p>Wir nehmen für den Moment an, dass wir einen numerischen Algorithmus
kennen mit dem wir eine Menge von <span class="math">\(A\)</span>-orthogonalen Vektoren
<span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> konstruieren können. Wie man
diese Menge konkret erhält werden wir uns im Anschluss erschließen. Sei
also nun im Folgenden <span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> eine
gegebene Menge von <span class="math">\(A\)</span>-orthogonalen Vektoren. Dann stellen wir uns die
Frage, wie die optimalen Schrittweiten <span class="math">\(\alpha_k &gt; 0\)</span> in
<a class="reference internal" href="#equation-eq-conjugate-descent">(1.36)</a> gewählt werden müssen, um in <span class="math">\(n\)</span> Schritten
das lokale Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> der Funktion <span class="math">\(F\)</span> zu erhalten.
Man beachte hierbei, dass wir nicht nur daran interessiert sind den
Punkt <span class="math">\(x^*\)</span> genügend gut zu approximieren, sondern wir fordern die
eindeutige Lösung des linearen Gleichungssystems <span class="math">\(Ax = b\)</span> in <span class="math">\(n\)</span>
Schritten zu finden, d.h., wir nehmen explizit <span class="math">\(x_n = x^*\)</span> an.</p>
<p>Um das lokale Minimum wirklich in <span class="math">\(n\)</span> Schritten zu erreichen müssen wir
fordern, dass wir in jede Richtung <span class="math">\(d_k\)</span> nur einmal gehen und der
entstehende Fehler <span class="math">\(e_{k+1}\)</span> <span class="math">\(A\)</span>-orthogonal hierzu ist. Das entspricht
der Forderung, dass man im entzerrten Problem auf der rechten Seite von
<a class="reference internal" href="#fig-conjugacy"><span class="std std-numref">Abb. 1.5</span></a> nur orthogonale Richtungen verwendet. Wir wollen
also folgende Eigenschaft erzwingen:</p>
<div class="math" id="equation-eq-req-error-conjugated">
<span class="eqno">(1.37)<a class="headerlink" href="#equation-eq-req-error-conjugated" title="Permalink to this equation">#</a></span>\[\langle Ae_{k+1}, d_k \rangle \ = \ 0.\]</div>
<p>Analog zur Idee der orthogonalen Richtungen in
<a class="reference internal" href="#ss-orthogonal-descent"><span class="std std-ref">Orthogonale Abstiegsrichtungen</span></a> können wir den Fehler <span class="math">\(e_{k+1}\)</span> in
<a class="reference internal" href="#equation-eq-req-error-conjugated">(1.37)</a> wieder entwickeln, um die optimale
Schrittweitenlänge <span class="math">\(\alpha_k &gt; 0\)</span> zu bestimmen</p>
<div class="math">
\[\begin{split}
0 \ &\overset{!}{=} \ \langle d_k, A e_{k+1} \rangle \ = \ \langle d_k, A(x_{k+1} - x^*) \rangle \ = \ \langle d_k, A(x_k + \alpha_k d_k - x^*) \rangle \\
\ &= \ \langle d_k, A(e_k + \alpha_k d_k) \rangle \ = \ \langle d_k, -r_k + \alpha_k Ad_k \rangle \ = \ \alpha_k \langle d_k, Ad_k \rangle - \langle d_k, r_k\rangle.
\end{split}\]</div>
<p>Da wir <span class="math">\(A\)</span> als positiv definit vorausgesetzt haben, können wir die
folgende Gleichung umstellen zu</p>
<div class="math" id="equation-eq-optimal-step-size-conjugated">
<span class="eqno">(1.38)<a class="headerlink" href="#equation-eq-optimal-step-size-conjugated" title="Permalink to this equation">#</a></span>\[\alpha_k \ = \ \frac{\langle d_k, r_k \rangle}{\langle d_k, A d_k \rangle}.\]</div>
<p>Im Gegensatz zur Idee der orthogonalen Richtungen in
<a class="reference internal" href="#equation-eq-optimal-step-size-orthogonal">(1.35)</a> lässt sich der Ausdruck in
<a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a> explizit berechnen und hängt nicht
von dem unbekannten lokalen Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> ab.
Zusammenfassend heißt das, dass wir aus der Bedingung, dass die
Abstiegsrichtung <span class="math">\(d_k \in \mathbb{R}^n\)</span> <span class="math">\(A\)</span>-orthogonal zum Fehlervektor
<span class="math">\(e_{k+1} \in \mathbb{R}^n\)</span> sein soll, eine Schrittlänge <span class="math">\(\alpha_k &gt; 0\)</span>
finden konnten, welche diese Bedingung erfüllt.</p>
<p>Andersherum könnte man fragen, welche Bedingung man aus der Optimalität
einer unbekannten Schrittlänge <span class="math">\(\alpha &gt; 0\)</span> folgern könnte. Dazu
betrachen wir wieder die notwendigen Optimalitätsbedingungen im totalen
Differential</p>
<div class="math">
\[\begin{split}
0 \ &\overset{!}{=} \ \frac{\mathrm{d}}{\mathrm{d}\alpha}F(x_{k+1}) \ = \ \langle  \nabla F(x_{k+1}), \frac{\mathrm{d}}{\mathrm{d}\alpha}x_{k+1} \rangle \ = \ \langle -r_{k+1}, \frac{\mathrm{d}}{\mathrm{d}\alpha} (x_k + \alpha d_k) \rangle \\
\ &= \ \langle -r_{k+1}, d_k \rangle \ = \ \langle Ae_{k+1}, d_k \rangle.
\end{split}\]</div>
<p>Wir erhalten also für die Optimalität der unbekannten Schrittlänge
<span class="math">\(\alpha &gt; 0\)</span>, dass die Abstiegsrichtung <span class="math">\(d_k \in \mathbb{R}^n\)</span> und der
Fehlervektor <span class="math">\(e_{k+1}\)</span> konjugiert bezüglich der Matrix <span class="math">\(A\)</span> sein müssen.
Das ist aber genau die Eigenschaft, die wir bereits in
<a class="reference internal" href="#equation-eq-req-error-conjugated">(1.37)</a> gefordert hatten. Wegen der positiven
Definitheit der Matrix <span class="math">\(A\)</span> können wir ebenfalls folgern, dass die
Forderung, dass <span class="math">\(e_{k+1}\)</span> und <span class="math">\(d_k\)</span> konjugiert bezüglich <span class="math">\(A\)</span> sind, bei
optimaler Schrittweite <span class="math">\(\alpha_k\)</span> aus
<a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a> in jedem Schritt zu einem Abstieg
in Richtung <span class="math">\(d_k\)</span> führt. Wir erhalten also für eine gegebene Menge von
<span class="math">\(A\)</span>-orthogonalen Vektoren <span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> ein
Iterationsverfahren mit optimalen Schrittlangen <span class="math">\(\alpha_k &gt; 0\)</span>, die wir
in <a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a> angeben können und die uns
einen Abstieg garantieren.</p>
<p>Zunächst benötigen wir die Einsicht aus folgendem Lemma, die es uns
ermöglicht eine Basis aus <span class="math">\(A\)</span>-orthogonalen Vektoren des <span class="math">\(\R^n\)</span> zu
betrachten.</p>
<div class="proof lemma admonition" id="lem:A-orthogonal_basis">
<p class="admonition-title"><span class="caption-number">Lemma 1.2 </span> (Basis von A-orthogonalen Vektoren)</p>
<section class="lemma-content" id="proof-content">
<p>Sei <span class="math">\(A \in \mathbb{R}^{n\times n}\)</span> eine symmetrische, positiv definite
Matrix und sei <span class="math">\(\lbrace d_0, \ldots, d_{n-1}\rbrace\)</span> eine Menge von
<span class="math">\(A\)</span>-orthogonalen Vektoren mit <span class="math">\(d_k \in \R^n \setminus \lbrace 0\rbrace\)</span>,
d.h., es gilt
<span class="math">\(\langle d_i, A d_j \rangle = \langle A d_i, d_j \rangle = 0\)</span> für alle
<span class="math">\(i \neq j\)</span>.</p>
<p>Dann bilden die Vektoren
<span class="math">\(d_0, \ldots, d_{n-1} \in \R^n \setminus \lbrace 0\rbrace\)</span> eine Basis
des <span class="math">\(\R^n\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. In den Übungsaufgaben zu zeigen. ◻</p>
</div>
<p>Folgender Satz zeigt uns, dass das Verfahren für eine gegebene Menge von
<span class="math">\(A\)</span>-konjugierten Vektoren in der Tat in <span class="math">\(n\)</span> Schritten das lokalen
Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> von <span class="math">\(F\)</span> erreicht.</p>
<div class="proof theorem admonition" id="thm:cg_convergence">
<p class="admonition-title"><span class="caption-number">Theorem 1.7 </span> (Konvergenz des CG-Verfahrens)</p>
<section class="theorem-content" id="proof-content">
<p>Sei eine Menge von <span class="math">\(A\)</span>-konjugierten Vektoren
<span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> mit
<span class="math">\(d_k \in \mathbb{R}^n / \lbrace 0 \rbrace\)</span> gegeben. Dann konvergiert das
Abstiegsverfahren in konjugierten Richtungen</p>
<div class="math" id="equation-eq-conjugate-descent-optimal">
<span class="eqno">(1.39)<a class="headerlink" href="#equation-eq-conjugate-descent-optimal" title="Permalink to this equation">#</a></span>\[x_{k+1} \ = \ x_k + \alpha_k d_k, \qquad \alpha_k \ = \ \frac{\langle r_k, d_k \rangle}{\langle d_k, Ad_k\rangle}\]</div>
<p>in genau <span class="math">\(n\)</span> Schritten gegen die Lösung <span class="math">\(x^* \in \mathbb{R}^n\)</span> des
quadratischen Optimierungsproblems <a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Für den Beweis der Konvergenz des Iterationsverfahrens
<a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a> betrachten wir zunächst den initialen
Fehler <span class="math">\(e_0\in \R^n\)</span> durch die Wahl eines beliebigen Startpunktes
<span class="math">\(x_0 \in \mathbb{R}^n\)</span>. Da wir <span class="math">\(A\)</span> als positiv definit angenommen haben
folgt mit <a class="reference internal" href="#lem:A-orthogonal_basis">Lemma 1.2</a>, dass die Menge
<span class="math">\(\lbrace d_k \rbrace_{k=0,\ldots,n-1}\)</span> eine Basis des <span class="math">\(\mathbb{R}^n\)</span>
bildet. Daher können wir den initialen Fehler <span class="math">\(e_0 \in \mathbb{R}^n\)</span> als
Linearkombination in dieser Basis darstellen als:</p>
<div class="math" id="equation-eq-error-basis">
<span class="eqno">(1.40)<a class="headerlink" href="#equation-eq-error-basis" title="Permalink to this equation">#</a></span>\[e_0 \ = \ \sum_{k=0}^{n-1} \delta_k d_k.\]</div>
<p>Um die unbekannten Koeffizienten <span class="math">\(\delta_k \in \mathbb{R}^n\)</span> zu
bestimmen können wir obige Gleichung nun jeweils von links mit einem
Vektor <span class="math">\(d_i^T A, i=0,\ldots,n-1\)</span> multiplizieren und erhalten so für
jeden Index eine Gleichung</p>
<div class="math">
\[\langle d_i^T A, e_0 \rangle \ = \ \langle d_i^T A, \sum_{k=0}^{n-1} \delta_k d_k \rangle \ = \ \sum_{k=0}^{n-1} \delta_k \langle d_i^T A, d_k \rangle \ = \ \delta_i \langle d_i^T A, d_i \rangle.\]</div>
<p>Hierbei haben wir die Bilinearität des Skalarproduktes in <span class="math">\(\mathbb{R}^n\)</span>
ausgenutzt und verwendet, dass die Vektoren
<span class="math">\(\lbrace d_k \rbrace_{k=0,\ldots,n-1}\)</span> konjugiert bezüglich der Matrix
<span class="math">\(A\)</span> sind. Damit können wir nach den unbekannten Koeffizienten
<span class="math">\(\delta_i \in \R\)</span> in jeder Gleichung auflösen und erhalten so einen
Ausdruck für die unbekannten Koeffizienten:</p>
<div class="math">
\[\delta_i \ = \ \frac{\langle d_i^T A, e_0 \rangle}{\langle d_i^T A, d_i \rangle}.\]</div>
<p>Man beachte, dass dieser Ausdruck wohldefiniert ist, da wir angenommen
haben, dass die Matrix <span class="math">\(A\)</span> positiv definit ist. Wir addieren eine Null
hinzu, indem wir Terme hinzufügen, die <span class="math">\(A\)</span>-konjugiert zur Richtung
<span class="math">\(d_i \in \R^n\)</span> sind:</p>
<div class="math" id="equation-eq-delta-coefficients">
<span class="eqno">(1.41)<a class="headerlink" href="#equation-eq-delta-coefficients" title="Permalink to this equation">#</a></span>\[\delta_i \ = \ \frac{\langle d_i^T A, e_0 \rangle}{\langle d_i^T A, d_i \rangle} + \underbrace{\frac{\langle d_i^T A, \sum_{k=0}^{i-1} \alpha_k d_k \rangle}{\langle d_i^T A, d_i \rangle}}_{=\: 0} \ = \ \frac{\langle d_i^T A, e_0 +\sum_{k=0}^{i-1} \alpha_k d_k \rangle}{\langle d_i^T A, d_i \rangle}.\]</div>
<p>Wir verwenden wieder den Trick, dass sich der Fehlervektor
<span class="math">\(e_{i+1} \in \mathbb{R}^n\)</span> entwickeln lässt zu
<span class="math">\(e_{i+1} = e_i + \alpha_i d_i\)</span> und somit können wir rekursiv herleiten,
dass</p>
<div class="math" id="equation-eq-error-recursive">
<span class="eqno">(1.42)<a class="headerlink" href="#equation-eq-error-recursive" title="Permalink to this equation">#</a></span>\[e_i \ = \ e_0 + \sum_{k=0}^{i-1} \alpha_k d_k.\]</div>
<p>Nun können wir die Gleichung <a class="reference internal" href="#equation-eq-error-recursive">(1.42)</a> in die Darstellung
der Koeffizienten <span class="math">\(\delta_i\)</span> in <a class="reference internal" href="#equation-eq-delta-coefficients">(1.41)</a> einsetzen
und erhalten:</p>
<div class="math">
\[\delta_i \ = \ \frac{\langle d_i^T A, e_0 +\sum_{k=0}^{i-1} \alpha_k d_k \rangle}{\langle d_i^T A, d_i \rangle} \ = \ \frac{\langle d_i^T A, e_i \rangle}{\langle d_i^T A, d_i \rangle} \ = \ \frac{\langle d_i, Ae_i \rangle}{\langle d_i^T A, d_i \rangle} \ = \ - \frac{\langle d_i, r_i \rangle}{\langle d_i^T A, d_i \rangle} \ = \ -\alpha_i.\]</div>
<p>Das bedeutet, dass die Koeffizienten <span class="math">\(\delta_i\)</span> in
<a class="reference internal" href="#equation-eq-delta-coefficients">(1.41)</a> gerade den negativen optimalen Schrittweiten
<span class="math">\(\alpha_i\)</span> in <a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a> entsprechen, d.h.,
<span class="math">\(\delta_i\)</span> = <span class="math">\(- \alpha_i\)</span>. Aus der Basisdarstellung des initialen
Fehlers <span class="math">\(e_0 = x_0 - x^*\)</span> in <a class="reference internal" href="#equation-eq-error-basis">(1.40)</a> können wir somit die
Behauptung des Satzes folgern:</p>
<div class="math">
\[x^* \ = \ x_0 - e_0 \ = \ x_0 - \sum_{k=0}^{n-1} \delta_k d_k \ = \ x_0 + \sum_{k=0}^{n-1} \alpha_k d_k \ = \ x_n.\]</div>
<p>◻</p>
</div>
<div class="proof remark admonition" id="rem:error_cg">
<p class="admonition-title"><span class="caption-number">Remark 1.14 </span> (Veränderung des Fehlers im Iterationsverfahren)</p>
<section class="remark-content" id="proof-content">
<p><br />
Anstatt im Beweis von <a class="reference internal" href="#thm:cg_convergence">Theorem 1.7</a> zu zeigen, dass sich
das eindeutige Minimum <span class="math">\(x^* \in \mathbb{R}^n\)</span> durch das
Iterationsverfahren zerlegen lässt, hätte man auch zeigen können, dass
der Fehlervektor <span class="math">\(e_i \in \mathbb{R}^n\)</span> in jedem Schritt des
Iterationsverfahren kleiner wird. Es gilt nämlich nach
<a class="reference internal" href="#equation-eq-error-recursive">(1.42)</a>:</p>
<div class="math">
\[e_i \ = \ e_0 + \sum_{k=0}^{i-1} \alpha_k d_k \ = \ \sum_{k=0}^{n-1}\delta_k d_k + \sum_{k=0}^{i-1}-\delta_k d_k \ = \ \sum_{k=i}^{n-1} \delta_k d_k.\]</div>
<p>Man sieht also, dass für eine wachsende Anzahl an Iterationen
<span class="math">\(i=0,\ldots,n-1\)</span> der Fehlerterm <span class="math">\(e_i \in \mathbb{R}^n\)</span> immer weniger
Terme hat, bis er schlussendlich ganz verschwindet.</p>
<p>Außerdem sagt es uns, dass der Abstieg mit konjugierten Richtungen in
dem Sinne optimal ist, als dass der Fehlerterm
<span class="math">\(e_i = \sum_{k=i}^{n-1} \delta_k d_k\)</span> keine Anteile der Richtungen
<span class="math">\(\lbrace d_j \rbrace_{j=0,\ldots,i-1}\)</span> mehr besitzt. Wir müssen also
nicht mehr entlang dieser Richtungen gehen, um zum lokalen Minimum
<span class="math">\(x^* \in \mathbb{R}^n\)</span> von <span class="math">\(F\)</span> zu gelangen. Aus Sicht der Numerik ist
das eine sehr schöne Eigenschaft, da wir nicht gezwungenermaßen <span class="math">\(n\)</span>
Iterationen des Abstiegsverfahrens <a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a>
durchführen müssen, sondern bereits nach <span class="math">\(k &lt; n\)</span> abbrechen können, um
eventuell eine gute Approximation des lokalen Minimums
<span class="math">\(x_k \approx x^* \in \mathbb{R}^n\)</span> zu erhalten. Dies spielt insbesondere
bei sehr großen Dimensionen <span class="math">\(n &gt;\!\!&gt; 1\)</span> eine wichtige Rolle.</p>
</section>
</div><div class="proof example admonition" id="example-9">
<p class="admonition-title"><span class="caption-number">Example 1.3 </span> (Konjugierte Abstiegsrichtungen)</p>
<section class="example-content" id="proof-content">
<p>Wir wollen im Folgenden ein Beispiel zur Durchführung eines
Abstiegsverfahrens mit gegebenen konjugierten Richtungen angeben. Seien
folgende Werte für das lineare Gleichungssystem <span class="math">\(Ax = b\)</span> gegeben:</p>
<div class="math">
\[A \ = \
\begin{pmatrix}
3 & 2\\
2 & 6
\end{pmatrix},
\quad b \ = \ 
\begin{pmatrix}
2\\
-8
\end{pmatrix}.\]</div>
<p>Als Startwert für unser Iterationsverfahren wählen wir
<span class="math">\(x_0 = (-2, 2)^T\)</span>. Wir nehmen eine Menge von zwei <span class="math">\(A\)</span>-orthogonalen
Vektoren <span class="math">\(d_0, d_1 \in \mathbb{R}^2 / \lbrace 0 \rbrace\)</span> als gegeben an
mit:</p>
<div class="math">
\[d_0 \ = \
\begin{pmatrix}
0\\
-1
\end{pmatrix},
\quad d_1 \ = \ 
\begin{pmatrix}
3\\
-1
\end{pmatrix}.\]</div>
<p>Wir sehen ein, dass die Vektoren <span class="math">\(d_0\)</span> und <span class="math">\(d_1\)</span> konjugiert bezüglich
der Matrix <span class="math">\(A\)</span> sind, denn es gilt:</p>
<div class="math">
\[\langle d_0, Ad_1 \rangle \ = \ (0,-1)\cdot
\begin{pmatrix}
3 & 2\\
2 & 6
\end{pmatrix}
\begin{pmatrix}
3\\
-1
\end{pmatrix} \ = \ (0,-1) \cdot
\begin{pmatrix}
7\\
0
\end{pmatrix} \ = \ 0.\]</div>
<p>Für den ersten Schritt des Iterationsverfahren berechnen wir zuerst das
aktuelle Residuum</p>
<div class="math">
\[r_0 \ = \ b - Ax_0 \ = \ 
\begin{pmatrix}
2 \\
-8
\end{pmatrix} - 
\begin{pmatrix}
3 & 2\\
2 & 6
\end{pmatrix}
\begin{pmatrix}
-2\\
2
\end{pmatrix}
\ = \
\begin{pmatrix}
2 \\
-8
\end{pmatrix} - 
\begin{pmatrix}
-2 \\
8
\end{pmatrix}
\ = \ 
\begin{pmatrix}
4 \\
-16
\end{pmatrix}.\]</div>
<p>Nun können wir die optimale Schrittweite <span class="math">\(\alpha_0 &gt; 0\)</span> für den ersten
Schritt durch den Ausdruck <a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a>
bestimmen mit:</p>
<div class="math">
\[\alpha_0 \ = \ \frac{\langle d_0, r_0\rangle}{\langle d_0, A d_0 \rangle} \ = \
\frac{8}{3}.\]</div>
<p>Hiermit können wir den ersten Abstieg durchführen und erhalten so den
nächsten Iterationspunkt</p>
<div class="math">
\[x_1 \ = \ x_0 + \alpha_0 d_0 \ = \ 
\begin{pmatrix}
-2 \\
- 2/3
\end{pmatrix}.\]</div>
<p>Wir wollen nur den zweiten Schritt des Verfahrens angehen und benötigen
wiederum das aktuelle Residuum</p>
<div class="math">
\[r_1 \ = \ b - Ax_1 \ = \ 
\begin{pmatrix}
28/3 \\
0
\end{pmatrix}.\]</div>
<p>Wir berechnen wieder die neue optimale Schrittweite mittels
<a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a>:</p>
<div class="math">
\[\alpha_1 \ = \ \frac{\langle d_1, r_1\rangle}{\langle d_1, A d_1 \rangle} \ = \
\frac{28}{21}.\]</div>
<p>Mit dieser können wir den letzten Abstiegsschritt für <span class="math">\(n=2\)</span> berechnen
und erhalten somit:</p>
<div class="math">
\[x_2 \ = \ x_1 + \alpha_1 d_1 \ = \
\begin{pmatrix}
2\\
-2
\end{pmatrix}
\ = \ x^*.\]</div>
</section>
</div><p>Der folgende Satz hilft uns zu verstehen, warum ein Abstiegsverfahren
mit konjugierten Richtungen besser funktioniert als das
Gradientenabstiegsverfahren in <a class="reference internal" href="02_02_NichtlineareOptimierung.html#ss-gradient-descent"><span class="std std-ref">Gradientenabstiegsverfahren</span></a>.</p>
<div class="proof theorem admonition" id="thm:residual_orthogonal">
<p class="admonition-title"><span class="caption-number">Theorem 1.8 </span> (Orthogonalität des Residuums)</p>
<section class="theorem-content" id="proof-content">
<p>Das Residuum <span class="math">\(r_{i+1} = b - Ax_{i+1}\)</span> des Abstiegsverfahren mit
konjugierten Richtungen in <a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a> ist
orthogonal zu allen bisherigen Abstiegsrichtungen <span class="math">\(d_j, j=0,\ldots,i\)</span>,
d.h.</p>
<div class="math">
\[\langle r_{i+1}, d_j \rangle \ = \ 0, \quad \text{ für alle } j=0,\ldots,i.\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Aus <a class="reference internal" href="#rem:error_cg">Remark 1.14</a> wissen wir, dass wir den Fehler <span class="math">\(e_{i+1}\)</span>
nach <span class="math">\(i\)</span> Iterationen des Abstiegsverfahrens angeben können als</p>
<div class="math">
\[e_{i+1} \ = \ \sum_{k=i+1}^{n-1} \delta_k d_k.\]</div>
<p>Wir können beide Seiten der Gleichung mit einem Zeilenvektor
<span class="math">\(-d_j^TA \in \R^n\)</span> für einen Index <span class="math">\(0 \leq j \leq i\)</span> von links
multiplizieren und erhalten damit:</p>
<div class="math">
\[-\langle d_j, Ae_{i+1} \rangle \ = \ - \sum_{k=i+1}^{n-1} \delta_k \underbrace{d_j Ad_k}_{=~0} \quad 
\Rightarrow \ \langle d_j, r_{i+1} \rangle \ = \ 0 \quad \text{ für alle } \ 0 \leq j \leq i.\]</div>
<p>◻</p>
</div>
<p>Man beachte, dass die Eigenschaft optimal bezüglich <strong>aller vorherigen
Abstiegsrichtungen</strong> nur für den Fall von konjugierten Richtungen
funktioniert und nicht im Fall des Gradientenabstiegsverfahren, wie wir
in <a class="reference internal" href="#ss-motivation"><span class="std std-ref">Motivation</span></a> gesehen haben. Hier war man nur optimal
bezüglich der <strong>letzten Abstiegsrichtung</strong> und nicht bezüglich aller
vorherigen Richtungen. Das resultiert in dem typischen Zickzack-Pfad
beim Abstieg, wie wir ihn in <a class="reference internal" href="#fig-zig-zag"><span class="std std-numref">Abb. 1.3</span></a> gesehen haben.</p>
</section>
<section id="konjugierte-gradienten">
<span id="ss-conjugated-gradients"></span><h2><span class="section-number">1.3.5. </span>Konjugierte Gradienten<a class="headerlink" href="#konjugierte-gradienten" title="Permalink to this heading">#</a></h2>
<p>Wir haben in <a class="reference internal" href="#ss-conjugated-descent"><span class="std std-ref">Konjugierte Abstiegsrichtungen</span></a> gesehen, dass wir ein
iteratives Abstiegsverfahren mit konjugierten Abstiegsrichtungen
<span class="math">\(\lbrace d_j \rbrace_{j=0,\ldots,n-1}\)</span> verwenden können, um in <span class="math">\(n\)</span>
Iterationen die eindeutige Lösung des quadratischen Minimierungsproblems
<a class="reference internal" href="#equation-eq-quadratic-problem">(1.31)</a> und somit die Lösung des linearen
Gleichungssystems <span class="math">\(Ax = b\)</span> zu erhalten. Bisher sind wir jedoch davon
ausgegangen, dass wir die Menge der konjugierten Vektoren
<span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span> bereits kennen. Um einen
Algorithmus angeben zu können müssen wir also noch ergründen, wie sich
diese Menge mit möglichst geringen numerischen Aufwand finden lässt.</p>
<p>Eine naheliegende Idee wäre es das <strong>Gram-Schmidtsche
Orthogonalisierungsverfahren</strong> so umzugestalten, dass wir eine Menge von
linear unabhängigen Vektoren <span class="math">\(\lbrace u_0,\ldots, u_{n-1} \rbrace\)</span> mit
<span class="math">\(u_k \in \R^n, k=0,\ldots,n-1\)</span> konjugieren bezüglich der Matrix <span class="math">\(A\)</span>.
Hierzu würde man die erste Abstiegsrichtung <span class="math">\(d_0 \in \R^n\)</span> des
Abstiegsverfahrens mit konjugierten Richtungen als den ersten Vektor der
Menge wählen, d.h., wir setzen <span class="math">\(d_0 = u_0\)</span>. Anschließend konstruieren
wir die nächste Abstiegsrichtung <span class="math">\(d_1\)</span> indem wir alle Komponenten von
<span class="math">\(u_1\)</span> entfernen, die nicht <span class="math">\(A\)</span>-orthogonal zu <span class="math">\(d_0\)</span> sind. Für die nächste
Abstiegsrichtung <span class="math">\(d_2\)</span> gehen wir analog vor, nur müssen wir darauf
achten alle Komponenten von <span class="math">\(u_2\)</span> zu entfernen, die nicht <span class="math">\(A\)</span>-orthogonal
zu <span class="math">\(d_0\)</span> und <span class="math">\(d_1\)</span> sind. Dieses Vorgehen lässt sich iterativ bis zum
Vektor <span class="math">\(d_{n-1}\)</span> fortführen und man erhält eine Menge von konjugierten
Vektoren <span class="math">\(\lbrace d_0, \ldots, d_{n-1} \rbrace\)</span>. Diese lassen sich in
geschlossener Form angeben als:</p>
<div class="math" id="equation-eq-directions-gram">
<span class="eqno">(1.43)<a class="headerlink" href="#equation-eq-directions-gram" title="Permalink to this equation">#</a></span>\[d_i \ = \ u_i + \sum_{k=0}^{i-1} \beta_{i,k} d_k, \quad i=1,\ldots,n-1.\]</div>
<p>Wir müssen jedoch die Koeffizienten <span class="math">\(\beta_{i,k}\)</span> so bestimmen, dass die
Vektoren <span class="math">\(d_i\)</span> konjugiert zu allen vorherigen Richtungsvektoren
<span class="math">\(d_j \in \R^n, 0 \leq j &lt; i\)</span> sind. Um diese Koeffizienten zu bestimmen
multiplizieren wir <a class="reference internal" href="#equation-eq-directions-gram">(1.43)</a> wieder von links mit einem
Zeilenvektor <span class="math">\(d_j^TA \in \R^n\)</span> für ein
<span class="math">\(j \in \lbrace 0,\ldots,i-1\rbrace\)</span> und erhalten</p>
<div class="math">
\[\begin{split}
&\langle d_j, A d_i \rangle \ = \ \langle d_j, A u_i \rangle + \sum_{k=0}^{i-1} \beta_{i,k} \langle d_j, A d_k \rangle \\
\Rightarrow \quad & 0 \ = \ \langle d_j, Au_i \rangle + \beta_{i,j}\langle d_j, A d_j \rangle \\\
\Rightarrow \quad & \beta_{i,j} \ = \ - \frac{\langle d_j, Au_i \rangle}{\langle d_j, Ad_j \rangle}.
\end{split}\]</div>
<p>Der Ausdruck für die Koeffizienten <span class="math">\(\beta_{i,j}\)</span> ist wohldefiniert, da
wir angenommen haben, dass <span class="math">\(A\)</span> eine symmetrische, positiv definite
Matrix ist. Eigentlich könnten wir jetzt zufrieden sein, da wir ein
Verfahren angeben können mit dem sich ein Abstiegsverfahren mit
konjugierten Richtungen konstruieren lässt.</p>
<p>Leider haben wir durch die Verwendung des Gram-Schmidtschen
Orthogonalisierungsverfahrens nichts gewonnen, da der numerische Aufwand
zur Berechnung der unbekannten Koeffizienten im besten Fall in
<span class="math">\(\mathcal{O}(n^3)\)</span> liegt, was genau so teuer ist wie eine Invertierung
der Matrix <span class="math">\(A\)</span>, zum Beispiel mit dem Eliminationsverfahren von Gauss
<span id="id7">[<a class="reference internal" href="../../references.html#id3" title="Daniel Tenbrinck and Tim Roith. Vorlesungsskript zur einführung in die numerik (ws 22/23) an der fau erlangen-nürnberg. URL: https://www.math.fau.de/wp-content/uploads/2023/05/tenbrinck_script_numerik.pdf (visited on 2023-05-23).">TR</a>]</span>.</p>
<p>Glücklicherweise gibt es eine Möglichkeit eine Menge von konjugierten
Abstiegsrichtungen im Laufe des Iterationsverfahren
<a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a> zu konstruieren ohne den numerischen
Rechenaufwand des modifizierten Gram-Schmidtschen
Orthogonalisierungsverfahren zu benötigen. Hierzu werden wir ähnlich mit
einer Menge von initialen Richtungsvektoren
<span class="math">\(\lbrace u_0, \ldots, u_{n-1}\rbrace\)</span> beginnen und diese geeignet
anpassen. Es stellt sich nämlich heraus, dass in jeder Iteration ein
Vektor existiert, der bereits <span class="math">\(A\)</span>-orthogonal zu allen vorherigen
Abstiegsrichtungen ist mit Ausnahme der letzten. Diese Aussage wird
durch folgendes Lemma präzisiert.</p>
<div class="proof lemma admonition" id="lem:residual_direction">
<p class="admonition-title"><span class="caption-number">Lemma 1.3 </span> (Eigenschaften des Residuums)</p>
<section class="lemma-content" id="proof-content">
<p>Für <span class="math">\(i \in \lbrace 1,\ldots,n-1 \rbrace\)</span> befinden wir uns im <span class="math">\((i+1)\)</span>-ten
Schritt des Abstiegsverfahrens mit konjugierten Richtungen in
<a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a> und
<span class="math">\(\lbrace d_0, \ldots, d_{i-1} \rbrace\)</span> ist eine Menge von
<span class="math">\(A\)</span>-orthogonalen Vektoren und <span class="math">\(u_i = r_i\)</span> sei ein initialer
Richtungsvektor für den aktuellen Iterationsschritt. Dann gilt für
<span class="math">\(r_{i+1} = b - A x_{i+1} = b - A (x_i + \alpha_i r_i)\)</span>:</p>
<div class="math">
\[\langle r_{i+1}, r_j \rangle \ = \ 0, \quad \text{ für alle } \ j=0,\ldots,i,\]</div>
<p>und außerdem auch</p>
<div class="math">
\[\langle r_{i+1}, Ad_j \rangle \ = \ 0, \quad \text{ für alle } \ j=0,\ldots,i-1.\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Wir definieren zuerst die lineare Hülle, die durch die <span class="math">\(A\)</span>-orthogonalen
Vektoren aufgespannt wird durch:</p>
<div class="math">
\[\mathcal{D}_i \ \coloneqq \ \operatorname{span}\lbrace d_0, \ldots, d_{i-1} \rbrace \subset \R^n.\]</div>
<p>Wir gehen in diesem Beweis konstruktiv vor und wollen in jedem Schritt
den unbekannten <span class="math">\(A\)</span>-orthogonalen Vektor <span class="math">\(d_i \in \R^n\)</span> aus dem aktuellen
Residuum <span class="math">\(r_i \in \R^n\)</span> und den vorigen Richtungsvektoren
<span class="math">\(\lbrace d_0, \dots, d_{i-1}\rbrace\)</span> konstruieren.</p>
<p>Wir wählen als erste Abstiegsrichtung <span class="math">\(d_0 = r_0\)</span> und erhalten damit:</p>
<div class="math">
\[\operatorname{span}\lbrace r_0 \rbrace \ = \ \operatorname{span}\lbrace d_0 \rbrace \ = \ \mathcal{D}_1.\]</div>
<p>Aus <a class="reference internal" href="#thm:residual_orthogonal">Theorem 1.8</a> wissen wir, dass <span class="math">\(r_1\)</span> orthogonal
zu <span class="math">\(d_0\)</span> steht und somit folgt auch schon:</p>
<div class="math">
\[\langle r_0, r_1 \rangle \ = \ \langle d_0, r_1 \rangle \ = \ 0.\]</div>
<p>Da wir den nächsten <span class="math">\(A\)</span>-orthogonalen Richtungsvektor <span class="math">\(d_1 \in \R^n\)</span> aus
dem aktuellen Residuum <span class="math">\(r_1\)</span> und dem Unterraum <span class="math">\(\mathcal{D}_1\)</span>
konstruieren wollen, können wir damit folgern:</p>
<div class="math">
\[\mathcal{D}_2 \ = \ \operatorname{span}\lbrace \mathcal{D}_1, d_1 \rbrace \ = \ \operatorname{span}\lbrace \mathcal{D}_1, r_1 \rbrace \ = \ \operatorname{span}\lbrace r_0, r_1 \rbrace.\]</div>
<p>Analog können wir nun für beliebiges <span class="math">\(i \in \lbrace 1,\ldots,n-1\rbrace\)</span>
folgern, dass</p>
<div class="math">
\[\mathcal{D}_i \ = \ \operatorname{span}\lbrace r_0,\ldots, r_{i-1} \rbrace.\]</div>
<p>Aus <a class="reference internal" href="#thm:residual_orthogonal">Theorem 1.8</a> wissen wir, dass
<span class="math">\(r_{i+1} \perp \mathcal{D}_{i+1}\)</span> und damit wissen wir schon, dass die
erste Aussage des Satzes gilt:</p>
<div class="math">
\[\langle r_{i+1}, r_j \rangle \ = \ 0, \quad \text{ für alle } j=0,\ldots,i.\]</div>
<p>Für die zweite Aussage des Lemmas drücken wir nun das Residuum <span class="math">\(r_i\)</span>
durch den Fehler <span class="math">\(e_i\)</span> aus und erhalten:</p>
<div class="math">
\[r_i \ = \ -Ae_i \ = \ -A(e_{i-1} + \alpha_{i-1}d_{i-1}) \ = \ \underbrace{r_{i-1}}_{\in \mathcal{D}_i} - \underbrace{\alpha_{i-1}Ad_{i-1}}_{\in A\mathcal{D}_i}.\]</div>
<p>Wir sehen also, dass
<span class="math">\(r_i \in \operatorname{span}\lbrace r_{i-1}, A d_i \rbrace\)</span>. Außerdem
wissen wir durch unsere Folgerungen oben, dass
<span class="math">\(r_{i-1} \in \mathcal{D}_i\)</span> und <span class="math">\(A d_{i-1} \in A\mathcal{D}_i\)</span> gilt.
Damit gilt aber schon</p>
<div class="math">
\[\mathcal{D}_{i+1} \ = \ \operatorname{span}\lbrace \mathcal{D}_i, r_i \rbrace \ = \ \operatorname{span}\lbrace \mathcal{D}_i, A \mathcal{D}_i \rbrace.\]</div>
<p>Wenn wir dies rekursiv entwickeln sehen wir interessanterweise ein, dass</p>
<div class="math">
\[\mathcal{D}_i \ = \ \operatorname{span}\lbrace d_0, Ad_0, \ldots, A^{i-1}d_0\rbrace\]</div>
<p>Nach <a class="reference internal" href="#thm:residual_orthogonal">Theorem 1.8</a> wissen wir jedoch auch, dass
<span class="math">\(r_{i+1} \perp \mathcal{D}_{i+1}\)</span> und somit muss gelten auch für den
Unterraum gelten <span class="math">\(r_{i+1} \perp A\mathcal{D}_i\)</span>. Und damit haben wir die
zweite Aussage des Satzes gezeigt, nämlich dass
<span class="math">\(r_{i+1} \perp_A \mathcal{D}_i\)</span> oder</p>
<div class="math">
\[\langle r_{i+1}, A d_j \rangle, \quad \text{ für alle } j=0,\ldots,i-1.\]</div>
<p>◻</p>
</div>
<p><a class="reference internal" href="#lem:residual_direction">Lemma 1.3</a> sagt aus, dass das Residuum <span class="math">\(r_i\)</span> ein
guter Ausgangspunkt für einen weiteren <span class="math">\(A\)</span>-orthogonalen Richtungsvektor
<span class="math">\(d_i \in \R^n\)</span> im Punkt <span class="math">\(x_i\)</span> ist, da es bereits zu allen bisherigen
<span class="math">\(A\)</span>-orthogonalen Richtungen <span class="math">\(d_0,\ldots,d_{i-2}\)</span> konjugiert bezüglich
der Matrix <span class="math">\(A\)</span> ist. Wir müssen also nur noch dafür sorgen, dass der
initiale Richtungsvektor <span class="math">\(r_i \in \R^n\)</span> <span class="math">\(A\)</span>-orthogonal zur letzten
Suchrichtung <span class="math">\(d_{i-1}\)</span> wird. Dies ist numerisch wesentlich günstiger als
einen beliebig gewählten Richtungsvektor
<span class="math">\(u_i \in \mathbb{R}^n / \lbrace 0 \rbrace\)</span> <span class="math">\(A\)</span>-orthogonal zu machen
bezüglich aller vorigen Richtungsvektoren <span class="math">\(d_0,\ldots,d_{i-1}\)</span>.</p>
<p>Wir wollen also im Folgenden das vollständige Abstiegsverfahren mit
konjugierten Richtungen angeben. Da die initialen Richtungen nun als
<span class="math">\(r_i = -\nabla F(x_i)\)</span> für alle Iterationen <span class="math">\(i=0,\ldots, n-1\)</span> gewählt
werden, nennt man dieses Verfahren auch das <strong>Abstiegsverfahren der
konjugierten Gradienten</strong>.</p>
<div class="proof theorem admonition" id="theorem-12">
<p class="admonition-title"><span class="caption-number">Theorem 1.9 </span> (Konjugation der Residuen bezüglich A)</p>
<section class="theorem-content" id="proof-content">
<p>Wir befinden uns im <span class="math">\((i+1)\)</span>-ten Schritt des Verfahrens der konjugierten
Gradienten für <span class="math">\(i=0,\ldots, n-1\)</span> in einem Punkt
<span class="math">\(x_{i+1} \in \mathbb{R}^n\)</span> und wir wählen als initialen Richtungsvektor
das aktuelle Residuum <span class="math">\(r_{i+1} \in \R^n\)</span>, welches nach
<a class="reference internal" href="#lem:residual_direction">Lemma 1.3</a> bereits <span class="math">\(A\)</span>-orthogonal zu fast allen
vorherigen Richtungsvektoren <span class="math">\(\lbrace d_0, \ldots, d_{i-1}\rbrace\)</span> ist.
Indem wir den neuen Richtungsvektor <span class="math">\(d_{i+1} \in \R^n\)</span> definieren als</p>
<div class="math" id="equation-eq-conjugated-gradient">
<span class="eqno">(1.44)<a class="headerlink" href="#equation-eq-conjugated-gradient" title="Permalink to this equation">#</a></span>\[d_{i+1} \ \coloneqq \ r_{i+1} + \beta_{i+1} d_i, \qquad \beta_{i+1} \ \coloneqq \ \frac{\langle r_{i+1}, r_{i+1} \rangle}{\langle r_i, r_i \rangle},\]</div>
<p>erhalten wir die Eigenschaft, dass dieser Richtungsvektor nun
<span class="math">\(A\)</span>-orthogonal zu allen bisherigen Richtungsvektoren des Verfahrens ist,
d.h.,</p>
<div class="math">
\[d_{i+1} \: \perp_A \: d_j, \quad j = 0,\ldots,i.\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Wir müssen den initialen Richtungsvektor
<span class="math">\(u_{i+1} \in \mathbb{R}^n / \lbrace 0 \rbrace\)</span> so modifizieren, dass der
resultierende Vektor <span class="math">\(d_{i+1}\)</span> konjugiert zu <span class="math">\(d_i\)</span> bezüglich der Matrix
<span class="math">\(A\)</span> ist. Mit dem modifizierten Gram-Schmidtschen
Orthogonalisierungsverfahren erhalten wir die Form</p>
<div class="math">
\[d_{i+1} \ = \ r_{i+1} + \beta_{i+1} d_i, \qquad \beta_{i+1} \ = \ - \frac{\langle r_{i+1}, Ad_i\rangle}{\langle d_i, Ad_i \rangle}.\]</div>
<p>Die neue Abstiegsrichtung <span class="math">\(d_{i+1} \in \mathbb{R}^n / \lbrace 0 \rbrace\)</span>
ist nach Konstruktion <span class="math">\(A\)</span>-orthogonal zu allen vorherigen
Abstiegsrichtungen <span class="math">\(\lbrace d_0, \ldots, d_i \rbrace\)</span>, jedoch wollen wir
den Koeffizienten <span class="math">\(\beta_{i+1}\)</span> noch näher charakterisieren im
Folgenden.</p>
<p>Aus dem Beweis von <a class="reference internal" href="#lem:residual_direction">Lemma 1.3</a> wissen wir, dass
wir das aktuelle Residuum ausdrücken können als</p>
<div class="math">
\[r_{i+1} \ = \ r_i - \alpha_iAd_i.\]</div>
<p>Wir multiplizieren diese Gleichung von links mit dem Zeilenvektor
<span class="math">\(r_{i+1}^T \in \R^n\)</span> und erhalten</p>
<div class="math">
\[\langle r_{i+1}, r_{i+1} \rangle \ = \ \langle r_{i+
1}, r_i \rangle - \alpha_i\langle r_{i+1}, Ad_i \rangle.\]</div>
<p>Wir wissen aus <a class="reference internal" href="#lem:residual_direction">Lemma 1.3</a> jedoch auch, dass
<span class="math">\(\langle r_{i+1}, r_i \rangle = 0\)</span> gilt und damit erhalten wir den
Ausdruck</p>
<div class="math">
\[-\frac{1}{\alpha_i}\langle r_{i+1}, r_{i+1} \rangle \ = \ \langle r_{i+1}, Ad_i \rangle.\]</div>
<p>Wenn wir nun die optimale Schrittweite
<span class="math">\(\alpha_i = \frac{\langle r_i, d_i \rangle}{\langle d_i, Ad_i \rangle}\)</span>
aus <a class="reference internal" href="#equation-eq-conjugate-descent-optimal">(1.39)</a> einsetzen erhalten wir für den
Koeffizienten <span class="math">\(\beta_{i+1}\)</span>:</p>
<div class="math">
\[\begin{split}
& \ \langle r_{i+1}, Ad_i \rangle  \ = \ -\frac{1}{\alpha_i}\langle r_{i+1}, r_{i+1} \rangle \ = \ -\frac{\langle d_i, Ad_i \rangle}{\langle r_i, d_i \rangle} \langle r_{i+1}, r_{i+1} \rangle \\
\Rightarrow & \ \frac{\langle r_{i+1}, r_{i+1}\rangle }{\langle r_i, d_i \rangle} \ = \ -\frac{\langle r_{i+1}, Ad_i \rangle}{\langle d_i, Ad_i \rangle} \ = \ \beta_{i+1}.
\end{split}\]</div>
<p>Schlussendlich können wir den Nenner in diesem Ausdruck noch weiter
umschreiben, da der letzte Richtungsvektor <span class="math">\(d_i \in \R^n\)</span> mit dem
modifizierten Gram-Schmidt Orthogonalisierungsverfahren auch ausgedrückt
werden kann als</p>
<div class="math">
\[\langle r_i, d_i \rangle \ = \ \langle r_i, r_i + \beta_i d_{i-1} \rangle \ = \ \langle r_i, r_i \rangle + \beta_i \underbrace{\langle r_i, d_{i-1}\rangle}_{=~0} \ = \ \langle r_i, r_i \rangle.\]</div>
<p>Das Skalarprodukt in obiger Gleichung verschwindet auf Grund von der
Aussage von <a class="reference internal" href="#thm:residual_orthogonal">Theorem 1.8</a> und somit erhalten wir
schlussendlich für den Koeffizienten <span class="math">\(\beta_{i+1}\)</span> den simplen Ausdruck:</p>
<div class="math">
\[\beta_{i+1} \ = \ \frac{\langle r_{i+1}, r_{i+1}\rangle}{\langle r_i, r_i \rangle}.\]</div>
<p>◻</p>
</div>
<p>Mit der Herleitung von <a class="reference internal" href="#equation-eq-conjugated-gradient">(1.44)</a> können wir nun einen
Algorithmus für das Abstiegsverfahren mit konjugierten Gradienten zum
Lösen eines Gleichungssystems <span class="math">\(Ax = b\)</span> angeben.</p>
<div class="proof algorithm admonition" id="alg:conjugated_gradient">
<p class="admonition-title"><span class="caption-number">Algorithm 1.4 </span> (Lineares konjugierte Gradientenverfahren)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>function</strong> <span class="math">\(x^*=\)</span><code class="docutils literal notranslate"><span class="pre">conjugateGradient</span></code><span class="math">\((A, b, x_0)\)</span></p>
<p># Initialisierung<br />
<span class="math">\(d_0 = r_0 = b - Ax_0\)</span></p>
<p># Führe genau <span class="math">\(n\)</span> Schritte durch<br />
<strong>for</strong> <span class="math">\(k = 0,...,n-1\)</span> <strong>do</strong><br />
<br />
    # Berechne Schrittweite<br />
    <span class="math">\(\alpha_k \ = \ \frac{\langle r_k, d_k\rangle}{\langle d_k, A d_k\rangle}\)</span><br />
<br />
    # Führe Abstiegsschritt durch<br />
    <span class="math">\(x_{k+1} \ = \ x_k + \alpha_k d_k\)</span><br />
<br />
    <strong>if</strong> <span class="math">\(k &lt; n-1\)</span> <strong>then</strong><br />
<br />
        # Berechne effizient neues Residuum<br />
        <span class="math">\(r_{k+1} \ = \ r_k - \alpha_k Ad_k\)</span><br />
<br />
        # Berechne Koeffizienten<br />
        <span class="math">\(\beta_{k+1} \ = \ \frac{\langle r_{k+1}, r_{k+1}\rangle}{\langle r_k, r_k \rangle}\)</span><br />
<br />
        # Berechne neue Abstiegsrichung mit Gram-Schmidt<br />
        <span class="math">\(d_{k+1} = r_{k+1} + \beta_{k+1} d_k\)</span><br />
<br />
    <strong>end if</strong><br />
<strong>end for</strong><br />
<br />
# Ausgabe des letzten Punktes<br />
<span class="math">\(x^* = x_{k+1}\)</span></p>
</section>
</div></section>
<section id="verallgemeinerung-fur-nichtlineare-optimierung">
<span id="id8"></span><h2><span class="section-number">1.3.6. </span>Verallgemeinerung für nichtlineare Optimierung<a class="headerlink" href="#verallgemeinerung-fur-nichtlineare-optimierung" title="Permalink to this heading">#</a></h2>
<p>Wir haben festgestellt, dass das konjugierte Gradientenverfahren in
Algorithmus <a class="reference internal" href="#alg:conjugated_gradient">Algorithm 1.4</a> als Minimierung der
konvexen quadratischen Funktion
<span class="math">\(F(x) \coloneqq \langle x, Ax \rangle - \langle x, b \rangle + c\)</span>
konzipiert ist, um das äquivalente lineare Gleichungssystem <span class="math">\(Ax = b\)</span> zu
lösen. Es ist natürlich zu fragen, ob wir diesen Ansatz anpassen können,
um allgemeine konvexe Zielfunktionen oder sogar allgemeine nichtlineare
Zielfunktionen <span class="math">\(F\)</span> zu minimieren.</p>
<p>Um eine nichtlineare Variante des konjugierte Gradientenverfahrens zu
erhalten, müssen wir einige Anpassungen vornehmen. Fletcher und Reeves
<span id="id9">[]</span> zeigten, dass man anstelle der explizit berechenbaren
Schrittweite <span class="math">\(\alpha_k\)</span> für das quadratische Probleme in
<a class="reference internal" href="#equation-eq-optimal-step-size-conjugated">(1.38)</a> zunächst einmal eine Schrittweite
wählen muss, die ein approximatives Minimum der nichtlinearen
Zielfunktion <span class="math">\(F\)</span> entlang der Suchrichtung <span class="math">\(d_k \in \R^n\)</span> identifiziert.
Darüber hinaus muss das bisher verwendete Residuum
<span class="math">\(r_k = b - Ax_x \in \R^n\)</span> für den Einsatz in der nichtlinearen
Optimierung durch den Gradienten der Zielfunktion <span class="math">\(F\)</span> ersetzt werden.</p>
<p>Diese beiden Änderungen führen zu dem folgenden Algorithmus für die
nichtlineare Optimierung.</p>
<div class="proof algorithm admonition" id="alg:nonlinear_conjugated_gradient">
<p class="admonition-title"><span class="caption-number">Algorithm 1.5 </span> (Nichtlineares konjugierte Gradientenverfahren)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>function</strong> <span class="math">\(x^*=\)</span><code class="docutils literal notranslate"><span class="pre">nonlinearConjugateGradient</span></code><span class="math">\((F, \nabla F, x_0, \alpha_0, \sigma)\)</span></p>
<p># Initialisierung<br />
<span class="math">\(d_0 = - \nabla F(x_0)\)</span></p>
<p><strong>while</strong> <span class="math">\(||\nabla F(x_k)||^2 &gt; \epsilon\)</span> <strong>do</strong><br />
<br />
    <strong>while</strong> <span class="math">\(F(x_k + \alpha_k d_k) &gt; F(x_k)\)</span> <strong>do</strong><br />
        # VerringereSchrittweite um Faktor <span class="math">\(\sigma\)</span><br />
        <span class="math">\(\alpha_k = \sigma\alpha_k\)</span><br />
    <strong>end while</strong><br />
<br />
    # Führe Abstiegsschritt durch<br />
    <span class="math">\(x_{k+1} \ = \ x_k + \alpha_k d_k\)</span><br />
<br />
    # Berechne Koeffizienten<br />
    <span class="math">\(\beta_{k+1} \ = \ \frac{\langle \nabla F(x_{k+1}), \nabla F(x_{k+1})\rangle}{\langle \nabla F(x_{k}), \nabla F(x_{k}) \rangle}\)</span><br />
<br />
    # Berechne neue Abstiegsrichung mit Gram-Schmidt<br />
    <span class="math">\(d_{k+1} = -\nabla F(x_{k+1}) + \beta_{k+1} d_k\)</span><br />
<br />
<strong>end while</strong></p>
<p># Ausgabe des letzten Punktes<br />
<span class="math">\(x^* = x_{k+1}\)</span></p>
</section>
</div><p>Man beachte, dass man für die Implementierung von Algorithmus
<a class="reference internal" href="#alg:nonlinear_conjugated_gradient">Algorithm 1.5</a> keinerlei Matrix-Vektor
Multiplikationen benötigt und man lediglich Auswertungen der
Zielfunktion <span class="math">\(F\)</span> und ihres Gradienten <span class="math">\(\nabla F\)</span> braucht. Für den Fall,
dass es sich bei der Zielfunktion <span class="math">\(F\)</span> um eine strikt konvexe,
quadratische Funktion handelt und wir in jedem Schritt die optimale
Schrittweite <span class="math">\(\alpha_k &gt; 0\)</span> bestimmten können, so entspricht Algorithmus
<a class="reference internal" href="#alg:nonlinear_conjugated_gradient">Algorithm 1.5</a> dem linearen konjugierte
Gradientenverfahren in Algorithmus <a class="reference internal" href="#alg:conjugated_gradient">Algorithm 1.4</a>.</p>
<p>In der Literatur hat sich unter Anderem eine Modikation des
nichtlinearen konjugierte Gradientenverfahrens von Fletcher und Reeves
etabliert, die sich in numerischen Experimenten häufig als robuster und
effizienter herausgestellt hat.</p>
<div class="proof remark admonition" id="remark-15">
<p class="admonition-title"><span class="caption-number">Remark 1.15 </span> (Polak-Ribière Variante)</p>
<section class="remark-content" id="proof-content">
<p>Bei der sogenannten Polak-Ribière Variante des Algorithmus
<a class="reference internal" href="#alg:nonlinear_conjugated_gradient">Algorithm 1.5</a> unterscheidet sich
hauptsächlich die Berechnung des Parameters <span class="math">\(\beta_{k+1} \in \R\)</span> in
<a class="reference internal" href="#equation-eq-conjugated-gradient">(1.44)</a> für die Anpassung der nächsten
Abstiegsrichtung. Hierbei wird der Faktor <span class="math">\(\beta_{k+1}\)</span> nämlich wie
folgt berechnet</p>
<div class="math">
\[\beta^{PR}_{k+1} \ \coloneqq \ \frac{\langle \nabla F(x_{k+1}), \nabla F(x_{k+1}) - \nabla F(x_{k}) \rangle}{\langle \nabla F(x_{k}), \nabla F(x_{k}) \rangle}.\]</div>
<p>Man sieht ein, dass im Falle einer strikt konvexen, quadratischen
Zielfunktion <span class="math">\(F\)</span> mit optimaler Schrittweitenwahl für die <span class="math">\(\alpha_k &gt; 0\)</span>
die Orthogonalitätsbedingung für sukzessive Gradienten aus
<a class="reference internal" href="#lem:optimale_schrittweite">Lemma 1.1</a> gilt mit</p>
<div class="math">
\[\langle \nabla F(x_{k+1}), \nabla F(x_k) \rangle \ = \ 0.\]</div>
<p>In diesem Fall stimmt der Faktor <span class="math">\(\beta^{PR}_{k+1}\)</span> mit dem Faktor
<span class="math">\(\beta_{k+1}\)</span> aus Algorithmus
<a class="reference internal" href="#alg:nonlinear_conjugated_gradient">Algorithm 1.5</a> überein. In allen anderen
Fällen hingegen unterscheiden sich die Faktoren im Allgemeinen und
führen so zu euben signifikant unterschiedlichen Konvergenzverhalten der
jeweiligen Abstiegsverfahren.</p>
</section>
</div></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./optimierung\02_NichtlineareOptimierung"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_02_NichtlineareOptimierung.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">zurück</p>
        <p class="prev-next-title"><span class="section-number">1.2. </span>Abstiegsverfahren</p>
      </div>
    </a>
    <a class="right-next"
       href="../03_NichtlineareOptimierung2/00_03_NichtlineareOptimierung2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title"><span class="section-number">1.4. </span>Wahl der Schrittweite</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Inhalt
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problemstellung">1.3.1. Problemstellung</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">1.3.2. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonale-abstiegsrichtungen">1.3.3. Orthogonale Abstiegsrichtungen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#konjugierte-abstiegsrichtungen">1.3.4. Konjugierte Abstiegsrichtungen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#konjugierte-gradienten">1.3.5. Konjugierte Gradienten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verallgemeinerung-fur-nichtlineare-optimierung">1.3.6. Verallgemeinerung für nichtlineare Optimierung</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Durch J. Laubmann, T. Roith, D. Tenbrinck
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <script type="application/json" class="js-hypothesis-config">{"assetRoot": "http://hypothesis.fau-mads.eu:3001/hypothesis/1.0.0-dummy-version/", "sidebarAppUrl":"http://hypothesis.fau-mads.eu:5000/app.html"}</script>
<script async="async" kind="hypothesis" src="http://hypothesis.fau-mads.eu:5000/embed.js"></script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>